Input args:
Dim: 2
seed: 1
seed_data: 10
/home/samwiq/snpla/seq-posterior-approx-w-nf-dev/mv_gaussian/low_dim_w_five_obs/lunarc
/home/samwiq/snpla/seq-posterior-approx-w-nf-dev
[1.0, 0.4965853037914095, 0.2465969639416065, 0.12245642825298195, 0.06081006262521797, 0.0301973834223185, 0.014995576820477717, 0.007446583070924344, 0.003697863716482932, 0.0018363047770289071]
start full training
Iteration: 1
optimizer_post_lr: [0.001]
prob_prior: 1.0
start update likelihood model
Epoch: 0, loss (training): 25.6391, loss (eval): 48.3265
Epoch: 1, loss (training): 19.7744, loss (eval): 21.5552
Epoch: 2, loss (training): 17.67, loss (eval): 18.3356
Epoch: 3, loss (training): 16.2847, loss (eval): 16.9781
Epoch: 4, loss (training): 15.0709, loss (eval): 15.6398
Epoch: 5, loss (training): 13.9663, loss (eval): 14.5085
Epoch: 6, loss (training): 13.1531, loss (eval): 13.5382
Epoch: 7, loss (training): 12.5468, loss (eval): 12.9305
Epoch: 8, loss (training): 11.974, loss (eval): 12.3654
Epoch: 9, loss (training): 11.5445, loss (eval): 11.7779
Epoch: 10, loss (training): 11.1489, loss (eval): 11.3469
Epoch: 11, loss (training): 10.8961, loss (eval): 11.2901
Epoch: 12, loss (training): 10.786, loss (eval): 10.8771
Epoch: 13, loss (training): 10.6102, loss (eval): 10.6804
Epoch: 14, loss (training): 10.4722, loss (eval): 10.6161
Epoch: 15, loss (training): 10.4724, loss (eval): 10.62
Epoch: 16, loss (training): 10.4944, loss (eval): 10.5726
Epoch: 17, loss (training): 10.3126, loss (eval): 10.4313
Epoch: 18, loss (training): 10.3583, loss (eval): 10.4917
Epoch: 19, loss (training): 10.3729, loss (eval): 10.563
Epoch: 20, loss (training): 10.3219, loss (eval): 10.4793
Epoch: 21, loss (training): 10.3145, loss (eval): 10.4737
Epoch: 22, loss (training): 10.3035, loss (eval): 10.4488
Epoch: 23, loss (training): 10.3075, loss (eval): 10.362
Epoch: 24, loss (training): 10.229, loss (eval): 10.3332
Epoch: 25, loss (training): 10.3118, loss (eval): 10.6293
Epoch: 26, loss (training): 10.2312, loss (eval): 10.4846
Epoch: 27, loss (training): 10.2678, loss (eval): 10.3553
Epoch: 28, loss (training): 10.1996, loss (eval): 10.8741
Epoch: 29, loss (training): 10.1631, loss (eval): 10.3527
Epoch: 30, loss (training): 10.1843, loss (eval): 10.461
Epoch: 31, loss (training): 10.2095, loss (eval): 10.3307
Epoch: 32, loss (training): 10.1496, loss (eval): 10.3598
Epoch: 33, loss (training): 10.1702, loss (eval): 10.3438
Epoch: 34, loss (training): 10.107, loss (eval): 10.3496
Epoch: 35, loss (training): 10.1795, loss (eval): 10.3318
Epoch: 36, loss (training): 10.115, loss (eval): 10.4049
Epoch: 37, loss (training): 10.2397, loss (eval): 10.4433
Epoch: 38, loss (training): 10.1615, loss (eval): 10.416
Epoch: 39, loss (training): 10.1272, loss (eval): 10.313
Epoch: 40, loss (training): 10.1547, loss (eval): 10.3572
Epoch: 41, loss (training): 10.1242, loss (eval): 10.2825
Epoch: 42, loss (training): 10.1383, loss (eval): 10.2933
Epoch: 43, loss (training): 10.1989, loss (eval): 10.2907
Epoch: 44, loss (training): 10.0926, loss (eval): 10.4696
Epoch: 45, loss (training): 10.1146, loss (eval): 10.3576
Epoch: 46, loss (training): 10.1133, loss (eval): 10.3808
Epoch: 47, loss (training): 10.1181, loss (eval): 10.3073
Epoch: 48, loss (training): 10.1635, loss (eval): 10.3414
Epoch: 49, loss (training): 10.071, loss (eval): 10.3381
Epoch: 50, loss (training): 10.0616, loss (eval): 10.327
Epoch: 51, loss (training): 10.1064, loss (eval): 10.3134
Epoch: 52, loss (training): 10.0569, loss (eval): 10.3488
Epoch: 53, loss (training): 10.1238, loss (eval): 10.4044
Epoch: 54, loss (training): 10.0474, loss (eval): 10.2773
Epoch: 55, loss (training): 10.0918, loss (eval): 10.3486
Epoch: 56, loss (training): 10.085, loss (eval): 10.3533
Epoch: 57, loss (training): 10.0893, loss (eval): 10.3083
Epoch: 58, loss (training): 10.0189, loss (eval): 10.2844
Epoch: 59, loss (training): 10.0873, loss (eval): 10.2749
Epoch: 60, loss (training): 10.0574, loss (eval): 10.3006
Epoch: 61, loss (training): 10.1314, loss (eval): 10.5196
Epoch: 62, loss (training): 10.0861, loss (eval): 10.3672
Epoch: 63, loss (training): 10.0736, loss (eval): 10.4691
Epoch: 64, loss (training): 10.0532, loss (eval): 10.4223
Epoch: 65, loss (training): 10.0977, loss (eval): 10.3087
Epoch: 66, loss (training): 10.0459, loss (eval): 10.4139
Epoch: 67, loss (training): 10.0102, loss (eval): 10.3082
Epoch: 68, loss (training): 10.0018, loss (eval): 10.3096
Epoch: 69, loss (training): 10.0466, loss (eval): 10.3571
Epoch: 70, loss (training): 10.0593, loss (eval): 10.2947
Epoch: 71, loss (training): 10.055, loss (eval): 10.4432
Epoch: 72, loss (training): 10.0841, loss (eval): 10.2835
Epoch: 73, loss (training): 10.0094, loss (eval): 10.362
Epoch: 74, loss (training): 9.9995, loss (eval): 10.286
start update posterior model from prior pred - hot start
Epoch: 0, loss (training): 3.6119, loss (eval): 7.0572
Epoch: 1, loss (training): 1.9571, loss (eval): 2.4218
Epoch: 2, loss (training): 1.3597, loss (eval): 1.5164
Epoch: 3, loss (training): 1.3579, loss (eval): 1.0651
Epoch: 4, loss (training): 0.928, loss (eval): 1.2643
Epoch: 5, loss (training): 0.949, loss (eval): 0.9206
Epoch: 6, loss (training): 0.9929, loss (eval): 1.3668
Epoch: 7, loss (training): 0.6748, loss (eval): 0.7208
Epoch: 8, loss (training): 0.6658, loss (eval): 0.6579
Epoch: 9, loss (training): 0.6291, loss (eval): 0.7743
start update posterior model
Epoch: 0, loss (training): 11.4228, loss (eval): 11.6111
Epoch: 1, loss (training): 11.4224, loss (eval): 11.4042
Epoch: 2, loss (training): 11.4184, loss (eval): 11.5256
Epoch: 3, loss (training): 11.4203, loss (eval): 11.3892
Epoch: 4, loss (training): 11.4078, loss (eval): 11.4199
Epoch: 5, loss (training): 11.4097, loss (eval): 11.3949
Epoch: 6, loss (training): 11.4074, loss (eval): 11.3971
Epoch: 7, loss (training): 11.406, loss (eval): 11.3997
Epoch: 8, loss (training): 11.4052, loss (eval): 11.3808
Epoch: 9, loss (training): 11.403, loss (eval): 11.4266
Epoch: 10, loss (training): 11.4052, loss (eval): 11.3883
Epoch: 11, loss (training): 11.4023, loss (eval): 11.3904
Epoch: 12, loss (training): 11.4005, loss (eval): 11.3987
Epoch: 13, loss (training): 11.4068, loss (eval): 11.4009
Epoch: 14, loss (training): 11.4, loss (eval): 11.4285
Epoch: 15, loss (training): 11.4052, loss (eval): 11.3886
Epoch: 16, loss (training): 11.4036, loss (eval): 11.3878
Epoch: 17, loss (training): 11.4092, loss (eval): 11.4202
Epoch: 18, loss (training): 11.4056, loss (eval): 11.4414
Epoch: 19, loss (training): 11.4005, loss (eval): 11.3995
Epoch: 20, loss (training): 11.4, loss (eval): 11.4217
Epoch: 21, loss (training): 11.3995, loss (eval): 11.4064
Epoch: 22, loss (training): 11.4031, loss (eval): 11.4026
Epoch: 23, loss (training): 11.3987, loss (eval): 11.3915
Epoch: 24, loss (training): 11.4056, loss (eval): 11.4057
Epoch: 25, loss (training): 11.3986, loss (eval): 11.4042
Epoch: 26, loss (training): 11.3999, loss (eval): 11.3842
Epoch: 27, loss (training): 11.4017, loss (eval): 11.4066
Early-stopping. Training converged after 28 epochs.
Iteration: 2
optimizer_post_lr: [0.00099]
prob_prior: 0.4965853037914095
start update likelihood model
Epoch: 0, loss (training): 10.2795, loss (eval): 10.168
Epoch: 1, loss (training): 10.2147, loss (eval): 10.1891
Epoch: 2, loss (training): 10.1966, loss (eval): 10.0953
Epoch: 3, loss (training): 10.183, loss (eval): 10.1434
Epoch: 4, loss (training): 10.1656, loss (eval): 10.1748
Epoch: 5, loss (training): 10.1794, loss (eval): 10.1195
Epoch: 6, loss (training): 10.1465, loss (eval): 10.1319
Epoch: 7, loss (training): 10.1394, loss (eval): 10.1208
Epoch: 8, loss (training): 10.1261, loss (eval): 10.1302
Epoch: 9, loss (training): 10.1223, loss (eval): 10.0848
Epoch: 10, loss (training): 10.13, loss (eval): 10.1278
Epoch: 11, loss (training): 10.1243, loss (eval): 10.1648
Epoch: 12, loss (training): 10.1458, loss (eval): 10.1197
Epoch: 13, loss (training): 10.1213, loss (eval): 10.1599
Epoch: 14, loss (training): 10.1138, loss (eval): 10.1848
Epoch: 15, loss (training): 10.1352, loss (eval): 10.1339
Epoch: 16, loss (training): 10.1012, loss (eval): 10.149
Epoch: 17, loss (training): 10.1002, loss (eval): 10.1287
Epoch: 18, loss (training): 10.0731, loss (eval): 10.1019
Epoch: 19, loss (training): 10.085, loss (eval): 10.1241
Epoch: 20, loss (training): 10.0917, loss (eval): 10.1766
Epoch: 21, loss (training): 10.1078, loss (eval): 10.1717
Epoch: 22, loss (training): 10.1159, loss (eval): 10.1697
Epoch: 23, loss (training): 10.0873, loss (eval): 10.0936
Epoch: 24, loss (training): 10.0823, loss (eval): 10.1734
Epoch: 25, loss (training): 10.0688, loss (eval): 10.1498
Epoch: 26, loss (training): 10.0715, loss (eval): 10.1473
Epoch: 27, loss (training): 10.0835, loss (eval): 10.1393
Epoch: 28, loss (training): 10.1013, loss (eval): 10.2222
Early-stopping. Training converged after 29 epochs.
start update posterior model
Epoch: 0, loss (training): 11.2827, loss (eval): 11.312
Epoch: 1, loss (training): 11.2837, loss (eval): 11.3051
Epoch: 2, loss (training): 11.2776, loss (eval): 11.2927
Epoch: 3, loss (training): 11.2767, loss (eval): 11.2628
Epoch: 4, loss (training): 11.2795, loss (eval): 11.2834
Epoch: 5, loss (training): 11.2817, loss (eval): 11.2681
Epoch: 6, loss (training): 11.2782, loss (eval): 11.2808
Epoch: 7, loss (training): 11.279, loss (eval): 11.2753
Epoch: 8, loss (training): 11.2772, loss (eval): 11.2763
Epoch: 9, loss (training): 11.2821, loss (eval): 11.2686
Epoch: 10, loss (training): 11.2772, loss (eval): 11.2768
Epoch: 11, loss (training): 11.281, loss (eval): 11.272
Epoch: 12, loss (training): 11.2786, loss (eval): 11.3014
Epoch: 13, loss (training): 11.2765, loss (eval): 11.2964
Epoch: 14, loss (training): 11.2784, loss (eval): 11.2884
Epoch: 15, loss (training): 11.2789, loss (eval): 11.2843
Epoch: 16, loss (training): 11.2778, loss (eval): 11.283
Epoch: 17, loss (training): 11.2816, loss (eval): 11.2757
Epoch: 18, loss (training): 11.2775, loss (eval): 11.28
Epoch: 19, loss (training): 11.2768, loss (eval): 11.2735
Epoch: 20, loss (training): 11.2766, loss (eval): 11.2953
Epoch: 21, loss (training): 11.276, loss (eval): 11.2846
Epoch: 22, loss (training): 11.2795, loss (eval): 11.2715
Early-stopping. Training converged after 23 epochs.
Iteration: 3
optimizer_post_lr: [0.0009801]
prob_prior: 0.2465969639416065
start update likelihood model
Epoch: 0, loss (training): 10.1981, loss (eval): 10.0841
Epoch: 1, loss (training): 10.1467, loss (eval): 10.1954
Epoch: 2, loss (training): 10.1248, loss (eval): 10.172
Epoch: 3, loss (training): 10.1115, loss (eval): 10.0903
Epoch: 4, loss (training): 10.0806, loss (eval): 10.1269
Epoch: 5, loss (training): 10.1143, loss (eval): 10.1368
Epoch: 6, loss (training): 10.0783, loss (eval): 10.1692
Epoch: 7, loss (training): 10.0699, loss (eval): 10.1114
Epoch: 8, loss (training): 10.074, loss (eval): 10.1059
Epoch: 9, loss (training): 10.0834, loss (eval): 10.254
Epoch: 10, loss (training): 10.0726, loss (eval): 10.1637
Epoch: 11, loss (training): 10.0589, loss (eval): 10.1631
Epoch: 12, loss (training): 10.0411, loss (eval): 10.1364
Epoch: 13, loss (training): 10.0416, loss (eval): 10.1392
Epoch: 14, loss (training): 10.0383, loss (eval): 10.1457
Epoch: 15, loss (training): 10.0579, loss (eval): 10.3857
Epoch: 16, loss (training): 10.0218, loss (eval): 10.1366
Epoch: 17, loss (training): 10.0281, loss (eval): 10.1053
Epoch: 18, loss (training): 10.0532, loss (eval): 10.1574
Epoch: 19, loss (training): 10.0255, loss (eval): 10.2424
Early-stopping. Training converged after 20 epochs.
start update posterior model
Epoch: 0, loss (training): 11.4065, loss (eval): 11.4578
Epoch: 1, loss (training): 11.4064, loss (eval): 11.4115
Epoch: 2, loss (training): 11.4067, loss (eval): 11.4003
Epoch: 3, loss (training): 11.4079, loss (eval): 11.4172
Epoch: 4, loss (training): 11.4071, loss (eval): 11.4138
Epoch: 5, loss (training): 11.4039, loss (eval): 11.4015
Epoch: 6, loss (training): 11.4046, loss (eval): 11.4023
Epoch: 7, loss (training): 11.4049, loss (eval): 11.3952
Epoch: 8, loss (training): 11.4076, loss (eval): 11.4055
Epoch: 9, loss (training): 11.4072, loss (eval): 11.4077
Epoch: 10, loss (training): 11.4094, loss (eval): 11.3986
Epoch: 11, loss (training): 11.4065, loss (eval): 11.3901
Epoch: 12, loss (training): 11.4047, loss (eval): 11.4039
Epoch: 13, loss (training): 11.4038, loss (eval): 11.3974
Epoch: 14, loss (training): 11.4075, loss (eval): 11.4231
Epoch: 15, loss (training): 11.4049, loss (eval): 11.4023
Epoch: 16, loss (training): 11.4081, loss (eval): 11.4085
Epoch: 17, loss (training): 11.4089, loss (eval): 11.3999
Epoch: 18, loss (training): 11.4086, loss (eval): 11.3975
Epoch: 19, loss (training): 11.4056, loss (eval): 11.4056
Epoch: 20, loss (training): 11.4082, loss (eval): 11.4058
Epoch: 21, loss (training): 11.4037, loss (eval): 11.3998
Epoch: 22, loss (training): 11.4027, loss (eval): 11.4046
Epoch: 23, loss (training): 11.4051, loss (eval): 11.4141
Epoch: 24, loss (training): 11.4041, loss (eval): 11.4048
Epoch: 25, loss (training): 11.4103, loss (eval): 11.4073
Epoch: 26, loss (training): 11.4082, loss (eval): 11.4016
Epoch: 27, loss (training): 11.4057, loss (eval): 11.3965
Epoch: 28, loss (training): 11.4049, loss (eval): 11.3968
Epoch: 29, loss (training): 11.4053, loss (eval): 11.4045
Epoch: 30, loss (training): 11.4052, loss (eval): 11.4024
Early-stopping. Training converged after 31 epochs.
Iteration: 4
optimizer_post_lr: [0.000970299]
prob_prior: 0.12245642825298195
start update likelihood model
Epoch: 0, loss (training): 10.1672, loss (eval): 10.1025
Epoch: 1, loss (training): 10.125, loss (eval): 10.0942
Epoch: 2, loss (training): 10.1224, loss (eval): 10.0885
Epoch: 3, loss (training): 10.101, loss (eval): 10.1788
Epoch: 4, loss (training): 10.1009, loss (eval): 10.1141
Epoch: 5, loss (training): 10.0831, loss (eval): 10.1038
Epoch: 6, loss (training): 10.0882, loss (eval): 10.1248
Epoch: 7, loss (training): 10.071, loss (eval): 10.0949
Epoch: 8, loss (training): 10.0632, loss (eval): 10.1225
Epoch: 9, loss (training): 10.068, loss (eval): 10.0728
Epoch: 10, loss (training): 10.0631, loss (eval): 10.2129
Epoch: 11, loss (training): 10.0782, loss (eval): 10.1516
Epoch: 12, loss (training): 10.0537, loss (eval): 10.1596
Epoch: 13, loss (training): 10.0401, loss (eval): 10.0832
Epoch: 14, loss (training): 10.0814, loss (eval): 10.1173
Epoch: 15, loss (training): 10.0442, loss (eval): 10.1574
Epoch: 16, loss (training): 10.0344, loss (eval): 10.137
Epoch: 17, loss (training): 10.04, loss (eval): 10.134
Epoch: 18, loss (training): 10.0808, loss (eval): 10.2008
Epoch: 19, loss (training): 10.0252, loss (eval): 10.1907
Epoch: 20, loss (training): 10.0431, loss (eval): 10.083
Epoch: 21, loss (training): 10.0455, loss (eval): 10.2685
Epoch: 22, loss (training): 10.0647, loss (eval): 10.1571
Epoch: 23, loss (training): 10.0647, loss (eval): 10.1557
Epoch: 24, loss (training): 10.0266, loss (eval): 10.11
Epoch: 25, loss (training): 10.0247, loss (eval): 10.1503
Epoch: 26, loss (training): 10.0085, loss (eval): 10.1312
Epoch: 27, loss (training): 10.0211, loss (eval): 10.0842
Epoch: 28, loss (training): 10.0353, loss (eval): 10.1197
Early-stopping. Training converged after 29 epochs.
start update posterior model
Epoch: 0, loss (training): 11.163, loss (eval): 11.1618
Epoch: 1, loss (training): 11.1632, loss (eval): 11.1678
Epoch: 2, loss (training): 11.159, loss (eval): 11.1509
Epoch: 3, loss (training): 11.1586, loss (eval): 11.161
Epoch: 4, loss (training): 11.1603, loss (eval): 11.1559
Epoch: 5, loss (training): 11.1602, loss (eval): 11.166
Epoch: 6, loss (training): 11.1625, loss (eval): 11.1549
Epoch: 7, loss (training): 11.1622, loss (eval): 11.1717
Epoch: 8, loss (training): 11.1592, loss (eval): 11.1602
Epoch: 9, loss (training): 11.1641, loss (eval): 11.1589
Epoch: 10, loss (training): 11.1616, loss (eval): 11.1626
Epoch: 11, loss (training): 11.1619, loss (eval): 11.1553
Epoch: 12, loss (training): 11.1626, loss (eval): 11.157
Epoch: 13, loss (training): 11.1613, loss (eval): 11.1712
Epoch: 14, loss (training): 11.1634, loss (eval): 11.208
Epoch: 15, loss (training): 11.1639, loss (eval): 11.1589
Epoch: 16, loss (training): 11.1612, loss (eval): 11.1599
Epoch: 17, loss (training): 11.1621, loss (eval): 11.1605
Epoch: 18, loss (training): 11.1608, loss (eval): 11.1664
Epoch: 19, loss (training): 11.1612, loss (eval): 11.1595
Epoch: 20, loss (training): 11.162, loss (eval): 11.165
Epoch: 21, loss (training): 11.1619, loss (eval): 11.1592
Early-stopping. Training converged after 22 epochs.
Iteration: 5
optimizer_post_lr: [0.0009605960099999999]
prob_prior: 0.06081006262521797
start update likelihood model
Epoch: 0, loss (training): 10.1451, loss (eval): 10.1904
Epoch: 1, loss (training): 10.0799, loss (eval): 10.1735
Epoch: 2, loss (training): 10.0793, loss (eval): 10.1986
Epoch: 3, loss (training): 10.0418, loss (eval): 10.2296
Epoch: 4, loss (training): 10.0151, loss (eval): 10.1872
Epoch: 5, loss (training): 10.0207, loss (eval): 10.2397
Epoch: 6, loss (training): 10.0081, loss (eval): 10.2083
Epoch: 7, loss (training): 10.0078, loss (eval): 10.1988
Epoch: 8, loss (training): 9.9835, loss (eval): 10.1879
Epoch: 9, loss (training): 9.996, loss (eval): 10.1894
Epoch: 10, loss (training): 10.0001, loss (eval): 10.2585
Epoch: 11, loss (training): 9.9766, loss (eval): 10.2017
Epoch: 12, loss (training): 9.9664, loss (eval): 10.2179
Epoch: 13, loss (training): 9.9794, loss (eval): 10.1944
Epoch: 14, loss (training): 9.9671, loss (eval): 10.2643
Epoch: 15, loss (training): 9.9966, loss (eval): 10.2689
Epoch: 16, loss (training): 9.9695, loss (eval): 10.1836
Epoch: 17, loss (training): 9.9498, loss (eval): 10.2099
Epoch: 18, loss (training): 9.9538, loss (eval): 10.1893
Epoch: 19, loss (training): 9.9647, loss (eval): 10.1819
Epoch: 20, loss (training): 9.9611, loss (eval): 10.2375
Early-stopping. Training converged after 21 epochs.
start update posterior model
Epoch: 0, loss (training): 11.0329, loss (eval): 11.0352
Epoch: 1, loss (training): 11.0336, loss (eval): 11.0327
Epoch: 2, loss (training): 11.0324, loss (eval): 11.0266
Epoch: 3, loss (training): 11.0353, loss (eval): 11.0357
Epoch: 4, loss (training): 11.0324, loss (eval): 11.0251
Epoch: 5, loss (training): 11.0354, loss (eval): 11.052
Epoch: 6, loss (training): 11.0313, loss (eval): 11.0264
Epoch: 7, loss (training): 11.0373, loss (eval): 11.0267
Epoch: 8, loss (training): 11.0344, loss (eval): 11.0274
Epoch: 9, loss (training): 11.0337, loss (eval): 11.0277
Epoch: 10, loss (training): 11.0349, loss (eval): 11.0506
Epoch: 11, loss (training): 11.0325, loss (eval): 11.0353
Epoch: 12, loss (training): 11.0315, loss (eval): 11.0305
Epoch: 13, loss (training): 11.0342, loss (eval): 11.0334
Epoch: 14, loss (training): 11.0311, loss (eval): 11.0283
Epoch: 15, loss (training): 11.0369, loss (eval): 11.0358
Epoch: 16, loss (training): 11.0316, loss (eval): 11.0383
Epoch: 17, loss (training): 11.0338, loss (eval): 11.0294
Epoch: 18, loss (training): 11.0326, loss (eval): 11.026
Epoch: 19, loss (training): 11.0319, loss (eval): 11.0339
Epoch: 20, loss (training): 11.0322, loss (eval): 11.0349
Epoch: 21, loss (training): 11.0306, loss (eval): 11.0362
Epoch: 22, loss (training): 11.0328, loss (eval): 11.0635
Epoch: 23, loss (training): 11.0355, loss (eval): 11.0263
Early-stopping. Training converged after 24 epochs.
Iteration: 6
optimizer_post_lr: [0.0009509900498999999]
prob_prior: 0.0301973834223185
start update likelihood model
Epoch: 0, loss (training): 10.136, loss (eval): 10.1204
Epoch: 1, loss (training): 10.0948, loss (eval): 10.1299
Epoch: 2, loss (training): 10.0612, loss (eval): 10.1434
Epoch: 3, loss (training): 10.0682, loss (eval): 10.1339
Epoch: 4, loss (training): 10.0841, loss (eval): 10.2232
Epoch: 5, loss (training): 10.0533, loss (eval): 10.1451
Epoch: 6, loss (training): 10.0524, loss (eval): 10.0954
Epoch: 7, loss (training): 10.0408, loss (eval): 10.0859
Epoch: 8, loss (training): 10.0165, loss (eval): 10.1062
Epoch: 9, loss (training): 10.0331, loss (eval): 10.1483
Epoch: 10, loss (training): 10.0171, loss (eval): 10.1449
Epoch: 11, loss (training): 10.0148, loss (eval): 10.1518
Epoch: 12, loss (training): 10.0107, loss (eval): 10.1273
Epoch: 13, loss (training): 10.0022, loss (eval): 10.1278
Epoch: 14, loss (training): 10.0056, loss (eval): 10.1348
Epoch: 15, loss (training): 10.0102, loss (eval): 10.1544
Epoch: 16, loss (training): 10.0037, loss (eval): 10.1629
Epoch: 17, loss (training): 9.9829, loss (eval): 10.1131
Epoch: 18, loss (training): 9.9902, loss (eval): 10.1143
Epoch: 19, loss (training): 9.9929, loss (eval): 10.1548
Epoch: 20, loss (training): 9.9933, loss (eval): 10.1189
Epoch: 21, loss (training): 10.0018, loss (eval): 10.1661
Epoch: 22, loss (training): 10.0087, loss (eval): 10.1287
Epoch: 23, loss (training): 9.9873, loss (eval): 10.1695
Epoch: 24, loss (training): 9.9893, loss (eval): 10.1542
Epoch: 25, loss (training): 9.9835, loss (eval): 10.1253
Epoch: 26, loss (training): 9.9816, loss (eval): 10.1697
Early-stopping. Training converged after 27 epochs.
start update posterior model
Epoch: 0, loss (training): 11.1497, loss (eval): 11.189
Epoch: 1, loss (training): 11.1488, loss (eval): 11.1436
Epoch: 2, loss (training): 11.1459, loss (eval): 11.1406
Epoch: 3, loss (training): 11.1483, loss (eval): 11.1559
Epoch: 4, loss (training): 11.1463, loss (eval): 11.1504
Epoch: 5, loss (training): 11.1453, loss (eval): 11.1605
Epoch: 6, loss (training): 11.149, loss (eval): 11.1482
Epoch: 7, loss (training): 11.1494, loss (eval): 11.1501
Epoch: 8, loss (training): 11.1498, loss (eval): 11.1427
Epoch: 9, loss (training): 11.146, loss (eval): 11.146
Epoch: 10, loss (training): 11.1483, loss (eval): 11.1433
Epoch: 11, loss (training): 11.1487, loss (eval): 11.1562
Epoch: 12, loss (training): 11.145, loss (eval): 11.1409
Epoch: 13, loss (training): 11.147, loss (eval): 11.1532
Epoch: 14, loss (training): 11.1458, loss (eval): 11.145
Epoch: 15, loss (training): 11.1486, loss (eval): 11.1459
Epoch: 16, loss (training): 11.1489, loss (eval): 11.1395
Epoch: 17, loss (training): 11.1475, loss (eval): 11.1379
Epoch: 18, loss (training): 11.1471, loss (eval): 11.1387
Epoch: 19, loss (training): 11.1494, loss (eval): 11.1507
Epoch: 20, loss (training): 11.146, loss (eval): 11.1572
Epoch: 21, loss (training): 11.1475, loss (eval): 11.1448
Epoch: 22, loss (training): 11.1446, loss (eval): 11.1385
Epoch: 23, loss (training): 11.1474, loss (eval): 11.1462
Epoch: 24, loss (training): 11.1488, loss (eval): 11.1517
Epoch: 25, loss (training): 11.1458, loss (eval): 11.1427
Epoch: 26, loss (training): 11.1461, loss (eval): 11.1491
Epoch: 27, loss (training): 11.1474, loss (eval): 11.1413
Epoch: 28, loss (training): 11.1464, loss (eval): 11.1476
Epoch: 29, loss (training): 11.1439, loss (eval): 11.1847
Epoch: 30, loss (training): 11.1457, loss (eval): 11.1503
Epoch: 31, loss (training): 11.1463, loss (eval): 11.1395
Epoch: 32, loss (training): 11.1464, loss (eval): 11.1498
Epoch: 33, loss (training): 11.1474, loss (eval): 11.1504
Epoch: 34, loss (training): 11.1462, loss (eval): 11.1399
Epoch: 35, loss (training): 11.1432, loss (eval): 11.137
Epoch: 36, loss (training): 11.1477, loss (eval): 11.1437
Epoch: 37, loss (training): 11.1458, loss (eval): 11.1506
Epoch: 38, loss (training): 11.1469, loss (eval): 11.1552
Epoch: 39, loss (training): 11.1448, loss (eval): 11.1424
Epoch: 40, loss (training): 11.1466, loss (eval): 11.1422
Epoch: 41, loss (training): 11.1455, loss (eval): 11.1509
Epoch: 42, loss (training): 11.1441, loss (eval): 11.1425
Epoch: 43, loss (training): 11.1434, loss (eval): 11.1459
Epoch: 44, loss (training): 11.1469, loss (eval): 11.1422
Epoch: 45, loss (training): 11.1488, loss (eval): 11.1394
Epoch: 46, loss (training): 11.1467, loss (eval): 11.1514
Epoch: 47, loss (training): 11.1455, loss (eval): 11.139
Epoch: 48, loss (training): 11.1454, loss (eval): 11.1399
Epoch: 49, loss (training): 11.1434, loss (eval): 11.1465
Epoch: 50, loss (training): 11.1445, loss (eval): 11.1449
Epoch: 51, loss (training): 11.1465, loss (eval): 11.1434
Epoch: 52, loss (training): 11.1483, loss (eval): 11.1598
Epoch: 53, loss (training): 11.1449, loss (eval): 11.1457
Epoch: 54, loss (training): 11.1436, loss (eval): 11.1413
Early-stopping. Training converged after 55 epochs.
Iteration: 7
optimizer_post_lr: [0.0009414801494009999]
prob_prior: 0.014995576820477717
start update likelihood model
Epoch: 0, loss (training): 10.0406, loss (eval): 10.3687
Epoch: 1, loss (training): 9.9855, loss (eval): 10.3242
Epoch: 2, loss (training): 9.9728, loss (eval): 10.2636
Epoch: 3, loss (training): 9.9525, loss (eval): 10.2532
Epoch: 4, loss (training): 9.9595, loss (eval): 10.2664
Epoch: 5, loss (training): 9.9331, loss (eval): 10.2695
Epoch: 6, loss (training): 9.9184, loss (eval): 10.2394
Epoch: 7, loss (training): 9.9133, loss (eval): 10.2206
Epoch: 8, loss (training): 9.9148, loss (eval): 10.2853
Epoch: 9, loss (training): 9.908, loss (eval): 10.2184
Epoch: 10, loss (training): 9.9114, loss (eval): 10.2396
Epoch: 11, loss (training): 9.9237, loss (eval): 10.288
Epoch: 12, loss (training): 9.8947, loss (eval): 10.269
Epoch: 13, loss (training): 9.8959, loss (eval): 10.2799
Epoch: 14, loss (training): 9.897, loss (eval): 10.2583
Epoch: 15, loss (training): 9.8847, loss (eval): 10.2591
Epoch: 16, loss (training): 9.9084, loss (eval): 10.3097
Epoch: 17, loss (training): 9.886, loss (eval): 10.3117
Epoch: 18, loss (training): 9.9052, loss (eval): 10.265
Epoch: 19, loss (training): 9.8852, loss (eval): 10.2619
Epoch: 20, loss (training): 9.893, loss (eval): 10.2963
Epoch: 21, loss (training): 9.8792, loss (eval): 10.3392
Epoch: 22, loss (training): 9.8983, loss (eval): 10.2989
Epoch: 23, loss (training): 9.856, loss (eval): 10.3062
Epoch: 24, loss (training): 9.8708, loss (eval): 10.2813
Epoch: 25, loss (training): 9.8611, loss (eval): 10.2696
Epoch: 26, loss (training): 9.8582, loss (eval): 10.3251
Epoch: 27, loss (training): 9.8622, loss (eval): 10.2841
Epoch: 28, loss (training): 9.8532, loss (eval): 10.2417
Early-stopping. Training converged after 29 epochs.
start update posterior model
Epoch: 0, loss (training): 11.3367, loss (eval): 11.3791
Epoch: 1, loss (training): 11.3316, loss (eval): 11.3314
Epoch: 2, loss (training): 11.3351, loss (eval): 11.3403
Epoch: 3, loss (training): 11.3328, loss (eval): 11.3351
Epoch: 4, loss (training): 11.3313, loss (eval): 11.3266
Epoch: 5, loss (training): 11.3306, loss (eval): 11.3369
Epoch: 6, loss (training): 11.3297, loss (eval): 11.3432
Epoch: 7, loss (training): 11.3302, loss (eval): 11.3388
Epoch: 8, loss (training): 11.3312, loss (eval): 11.3237
Epoch: 9, loss (training): 11.3316, loss (eval): 11.3266
Epoch: 10, loss (training): 11.3292, loss (eval): 11.3244
Epoch: 11, loss (training): 11.3325, loss (eval): 11.3392
Epoch: 12, loss (training): 11.3319, loss (eval): 11.3335
Epoch: 13, loss (training): 11.332, loss (eval): 11.3264
Epoch: 14, loss (training): 11.3329, loss (eval): 11.3293
Epoch: 15, loss (training): 11.3306, loss (eval): 11.3288
Epoch: 16, loss (training): 11.3324, loss (eval): 11.3289
Epoch: 17, loss (training): 11.3326, loss (eval): 11.3283
Epoch: 18, loss (training): 11.3312, loss (eval): 11.3355
Epoch: 19, loss (training): 11.3294, loss (eval): 11.3328
Epoch: 20, loss (training): 11.3319, loss (eval): 11.3263
Epoch: 21, loss (training): 11.3316, loss (eval): 11.3355
Epoch: 22, loss (training): 11.3313, loss (eval): 11.3316
Epoch: 23, loss (training): 11.3319, loss (eval): 11.3287
Epoch: 24, loss (training): 11.3321, loss (eval): 11.3363
Epoch: 25, loss (training): 11.332, loss (eval): 11.3281
Epoch: 26, loss (training): 11.3304, loss (eval): 11.323
Epoch: 27, loss (training): 11.3312, loss (eval): 11.3232
Epoch: 28, loss (training): 11.3305, loss (eval): 11.3335
Epoch: 29, loss (training): 11.3305, loss (eval): 11.3417
Epoch: 30, loss (training): 11.3308, loss (eval): 11.3396
Epoch: 31, loss (training): 11.33, loss (eval): 11.3278
Epoch: 32, loss (training): 11.3309, loss (eval): 11.3271
Epoch: 33, loss (training): 11.333, loss (eval): 11.3266
Epoch: 34, loss (training): 11.3303, loss (eval): 11.3451
Epoch: 35, loss (training): 11.3326, loss (eval): 11.3228
Epoch: 36, loss (training): 11.332, loss (eval): 11.3267
Epoch: 37, loss (training): 11.3291, loss (eval): 11.332
Epoch: 38, loss (training): 11.3298, loss (eval): 11.3331
Epoch: 39, loss (training): 11.3301, loss (eval): 11.3254
Epoch: 40, loss (training): 11.3305, loss (eval): 11.3265
Epoch: 41, loss (training): 11.3311, loss (eval): 11.321
Epoch: 42, loss (training): 11.3318, loss (eval): 11.3438
Epoch: 43, loss (training): 11.3298, loss (eval): 11.3281
Epoch: 44, loss (training): 11.3303, loss (eval): 11.3202
Epoch: 45, loss (training): 11.3303, loss (eval): 11.3246
Epoch: 46, loss (training): 11.3329, loss (eval): 11.3262
Epoch: 47, loss (training): 11.3315, loss (eval): 11.3315
Epoch: 48, loss (training): 11.3335, loss (eval): 11.3311
Epoch: 49, loss (training): 11.3319, loss (eval): 11.3249
Epoch: 50, loss (training): 11.3303, loss (eval): 11.3344
Epoch: 51, loss (training): 11.3319, loss (eval): 11.3372
Epoch: 52, loss (training): 11.3292, loss (eval): 11.3229
Epoch: 53, loss (training): 11.3306, loss (eval): 11.328
Epoch: 54, loss (training): 11.3346, loss (eval): 11.3321
Epoch: 55, loss (training): 11.3332, loss (eval): 11.327
Epoch: 56, loss (training): 11.3292, loss (eval): 11.3314
Epoch: 57, loss (training): 11.3325, loss (eval): 11.3236
Epoch: 58, loss (training): 11.3305, loss (eval): 11.3258
Epoch: 59, loss (training): 11.3299, loss (eval): 11.3297
Epoch: 60, loss (training): 11.3315, loss (eval): 11.3282
Epoch: 61, loss (training): 11.3301, loss (eval): 11.3268
Epoch: 62, loss (training): 11.3295, loss (eval): 11.326
Epoch: 63, loss (training): 11.3318, loss (eval): 11.333
Early-stopping. Training converged after 64 epochs.
Iteration: 8
optimizer_post_lr: [0.0009320653479069899]
prob_prior: 0.007446583070924344
start update likelihood model
Epoch: 0, loss (training): 10.2405, loss (eval): 10.6264
Epoch: 1, loss (training): 10.1828, loss (eval): 10.58
Epoch: 2, loss (training): 10.1574, loss (eval): 10.6047
Epoch: 3, loss (training): 10.15, loss (eval): 10.7073
Epoch: 4, loss (training): 10.1627, loss (eval): 10.6239
Epoch: 5, loss (training): 10.1327, loss (eval): 10.5765
Epoch: 6, loss (training): 10.1148, loss (eval): 10.6272
Epoch: 7, loss (training): 10.1185, loss (eval): 10.6061
Epoch: 8, loss (training): 10.1066, loss (eval): 10.6174
Epoch: 9, loss (training): 10.1008, loss (eval): 10.6184
Epoch: 10, loss (training): 10.0849, loss (eval): 10.5724
Epoch: 11, loss (training): 10.0776, loss (eval): 10.5875
Epoch: 12, loss (training): 10.0892, loss (eval): 10.5878
Epoch: 13, loss (training): 10.087, loss (eval): 10.5981
Epoch: 14, loss (training): 10.1082, loss (eval): 10.5716
Epoch: 15, loss (training): 10.083, loss (eval): 10.5952
Epoch: 16, loss (training): 10.0868, loss (eval): 10.5784
Epoch: 17, loss (training): 10.0756, loss (eval): 10.5896
Epoch: 18, loss (training): 10.0821, loss (eval): 10.5686
Epoch: 19, loss (training): 10.0804, loss (eval): 10.6247
Epoch: 20, loss (training): 10.0714, loss (eval): 10.5679
Epoch: 21, loss (training): 10.0564, loss (eval): 10.5978
Epoch: 22, loss (training): 10.0591, loss (eval): 10.563
Epoch: 23, loss (training): 10.0426, loss (eval): 10.5926
Epoch: 24, loss (training): 10.0614, loss (eval): 10.6269
Epoch: 25, loss (training): 10.0641, loss (eval): 10.6708
Epoch: 26, loss (training): 10.0632, loss (eval): 10.6252
Epoch: 27, loss (training): 10.0475, loss (eval): 10.6384
Epoch: 28, loss (training): 10.0438, loss (eval): 10.5714
Epoch: 29, loss (training): 10.0362, loss (eval): 10.6342
Epoch: 30, loss (training): 10.0386, loss (eval): 10.6448
Epoch: 31, loss (training): 10.0471, loss (eval): 10.6467
Epoch: 32, loss (training): 10.0448, loss (eval): 10.6548
Epoch: 33, loss (training): 10.0194, loss (eval): 10.6106
Epoch: 34, loss (training): 10.0216, loss (eval): 10.6504
Epoch: 35, loss (training): 10.023, loss (eval): 10.5884
Epoch: 36, loss (training): 10.0143, loss (eval): 10.601
Epoch: 37, loss (training): 10.024, loss (eval): 10.663
Epoch: 38, loss (training): 10.0327, loss (eval): 10.5826
Epoch: 39, loss (training): 10.0148, loss (eval): 10.6114
Epoch: 40, loss (training): 10.0067, loss (eval): 10.6033
Epoch: 41, loss (training): 10.0136, loss (eval): 10.6252
Early-stopping. Training converged after 42 epochs.
start update posterior model
Epoch: 0, loss (training): 11.1644, loss (eval): 11.2629
Epoch: 1, loss (training): 11.1577, loss (eval): 11.1509
Epoch: 2, loss (training): 11.1581, loss (eval): 11.1545
Epoch: 3, loss (training): 11.158, loss (eval): 11.1644
Epoch: 4, loss (training): 11.1581, loss (eval): 11.177
Epoch: 5, loss (training): 11.1611, loss (eval): 11.1571
Epoch: 6, loss (training): 11.1585, loss (eval): 11.1555
Epoch: 7, loss (training): 11.1568, loss (eval): 11.1573
Epoch: 8, loss (training): 11.1589, loss (eval): 11.1599
Epoch: 9, loss (training): 11.1591, loss (eval): 11.1587
Epoch: 10, loss (training): 11.1554, loss (eval): 11.1515
Epoch: 11, loss (training): 11.1562, loss (eval): 11.1574
Epoch: 12, loss (training): 11.1576, loss (eval): 11.1592
Epoch: 13, loss (training): 11.1553, loss (eval): 11.1514
Epoch: 14, loss (training): 11.1588, loss (eval): 11.1566
Epoch: 15, loss (training): 11.1566, loss (eval): 11.1536
Epoch: 16, loss (training): 11.1579, loss (eval): 11.1745
Epoch: 17, loss (training): 11.1577, loss (eval): 11.1535
Epoch: 18, loss (training): 11.1582, loss (eval): 11.1581
Epoch: 19, loss (training): 11.1561, loss (eval): 11.1489
Epoch: 20, loss (training): 11.1578, loss (eval): 11.1585
Epoch: 21, loss (training): 11.1572, loss (eval): 11.1505
Epoch: 22, loss (training): 11.1571, loss (eval): 11.1618
Epoch: 23, loss (training): 11.1572, loss (eval): 11.1491
Epoch: 24, loss (training): 11.1564, loss (eval): 11.1575
Epoch: 25, loss (training): 11.1561, loss (eval): 11.1588
Epoch: 26, loss (training): 11.1577, loss (eval): 11.1506
Epoch: 27, loss (training): 11.1586, loss (eval): 11.152
Epoch: 28, loss (training): 11.156, loss (eval): 11.1549
Epoch: 29, loss (training): 11.1573, loss (eval): 11.1595
Epoch: 30, loss (training): 11.157, loss (eval): 11.1584
Epoch: 31, loss (training): 11.1557, loss (eval): 11.1525
Epoch: 32, loss (training): 11.1571, loss (eval): 11.1784
Epoch: 33, loss (training): 11.157, loss (eval): 11.1513
Epoch: 34, loss (training): 11.1578, loss (eval): 11.1579
Epoch: 35, loss (training): 11.1595, loss (eval): 11.1654
Epoch: 36, loss (training): 11.1588, loss (eval): 11.1552
Epoch: 37, loss (training): 11.1559, loss (eval): 11.1533
Epoch: 38, loss (training): 11.1567, loss (eval): 11.1593
Early-stopping. Training converged after 39 epochs.
Iteration: 9
optimizer_post_lr: [0.00092274469442792]
prob_prior: 0.003697863716482932
start update likelihood model
Epoch: 0, loss (training): 10.2521, loss (eval): 10.4364
Epoch: 1, loss (training): 10.1706, loss (eval): 10.4065
Epoch: 2, loss (training): 10.1315, loss (eval): 10.4422
Epoch: 3, loss (training): 10.107, loss (eval): 10.4038
Epoch: 4, loss (training): 10.0889, loss (eval): 10.384
Epoch: 5, loss (training): 10.1038, loss (eval): 10.4262
Epoch: 6, loss (training): 10.0829, loss (eval): 10.3995
Epoch: 7, loss (training): 10.068, loss (eval): 10.4
Epoch: 8, loss (training): 10.0576, loss (eval): 10.4301
Epoch: 9, loss (training): 10.0532, loss (eval): 10.3622
Epoch: 10, loss (training): 10.0524, loss (eval): 10.3837
Epoch: 11, loss (training): 10.0403, loss (eval): 10.3958
Epoch: 12, loss (training): 10.0511, loss (eval): 10.4195
Epoch: 13, loss (training): 10.0504, loss (eval): 10.4412
Epoch: 14, loss (training): 10.0252, loss (eval): 10.4013
Epoch: 15, loss (training): 10.043, loss (eval): 10.4099
Epoch: 16, loss (training): 10.0211, loss (eval): 10.3873
Epoch: 17, loss (training): 10.0111, loss (eval): 10.3768
Epoch: 18, loss (training): 10.0182, loss (eval): 10.4168
Epoch: 19, loss (training): 10.0129, loss (eval): 10.4294
Epoch: 20, loss (training): 10.0202, loss (eval): 10.3966
Epoch: 21, loss (training): 10.0065, loss (eval): 10.3845
Epoch: 22, loss (training): 9.9992, loss (eval): 10.4313
Epoch: 23, loss (training): 10.0167, loss (eval): 10.4276
Epoch: 24, loss (training): 9.9959, loss (eval): 10.428
Epoch: 25, loss (training): 9.9895, loss (eval): 10.4593
Epoch: 26, loss (training): 9.9936, loss (eval): 10.4527
Epoch: 27, loss (training): 9.9928, loss (eval): 10.4677
Epoch: 28, loss (training): 9.9787, loss (eval): 10.4167
Early-stopping. Training converged after 29 epochs.
start update posterior model
Epoch: 0, loss (training): 11.5469, loss (eval): 11.5685
Epoch: 1, loss (training): 11.5405, loss (eval): 11.5411
Epoch: 2, loss (training): 11.5445, loss (eval): 11.5367
Epoch: 3, loss (training): 11.5409, loss (eval): 11.5334
Epoch: 4, loss (training): 11.5411, loss (eval): 11.5373
Epoch: 5, loss (training): 11.543, loss (eval): 11.5314
Epoch: 6, loss (training): 11.5407, loss (eval): 11.5403
Epoch: 7, loss (training): 11.5406, loss (eval): 11.5363
Epoch: 8, loss (training): 11.5434, loss (eval): 11.5636
Epoch: 9, loss (training): 11.5411, loss (eval): 11.5342
Epoch: 10, loss (training): 11.5429, loss (eval): 11.5345
Epoch: 11, loss (training): 11.5436, loss (eval): 11.5459
Epoch: 12, loss (training): 11.5406, loss (eval): 11.5425
Epoch: 13, loss (training): 11.5414, loss (eval): 11.541
Epoch: 14, loss (training): 11.5402, loss (eval): 11.5369
Epoch: 15, loss (training): 11.5402, loss (eval): 11.5412
Epoch: 16, loss (training): 11.5428, loss (eval): 11.5449
Epoch: 17, loss (training): 11.5399, loss (eval): 11.5441
Epoch: 18, loss (training): 11.5428, loss (eval): 11.5403
Epoch: 19, loss (training): 11.5406, loss (eval): 11.5404
Epoch: 20, loss (training): 11.5428, loss (eval): 11.5544
Epoch: 21, loss (training): 11.5419, loss (eval): 11.5316
Epoch: 22, loss (training): 11.5401, loss (eval): 11.5365
Epoch: 23, loss (training): 11.5414, loss (eval): 11.5436
Epoch: 24, loss (training): 11.5419, loss (eval): 11.5505
Early-stopping. Training converged after 25 epochs.
Iteration: 10
optimizer_post_lr: [0.0009135172474836408]
prob_prior: 0.0018363047770289071
start update likelihood model
Epoch: 0, loss (training): 10.1655, loss (eval): 10.3836
Epoch: 1, loss (training): 10.3274, loss (eval): 10.3078
Epoch: 2, loss (training): 10.1547, loss (eval): 10.4072
Epoch: 3, loss (training): 10.0883, loss (eval): 10.3272
Epoch: 4, loss (training): 10.0852, loss (eval): 10.3474
Epoch: 5, loss (training): 10.0532, loss (eval): 10.3333
Epoch: 6, loss (training): 10.0446, loss (eval): 10.2999
Epoch: 7, loss (training): 10.0453, loss (eval): 10.3556
Epoch: 8, loss (training): 10.0348, loss (eval): 10.3922
Epoch: 9, loss (training): 10.0279, loss (eval): 10.3231
Epoch: 10, loss (training): 10.0083, loss (eval): 10.3183
Epoch: 11, loss (training): 10.023, loss (eval): 10.3652
Epoch: 12, loss (training): 10.0177, loss (eval): 10.3628
Epoch: 13, loss (training): 10.0234, loss (eval): 10.376
Epoch: 14, loss (training): 9.9908, loss (eval): 10.3291
Epoch: 15, loss (training): 10.0045, loss (eval): 10.3551
Epoch: 16, loss (training): 9.9817, loss (eval): 10.3392
Epoch: 17, loss (training): 9.9807, loss (eval): 10.3982
Epoch: 18, loss (training): 9.9717, loss (eval): 10.3734
Epoch: 19, loss (training): 9.9658, loss (eval): 10.3483
Epoch: 20, loss (training): 9.9658, loss (eval): 10.3386
Epoch: 21, loss (training): 9.9769, loss (eval): 10.326
Epoch: 22, loss (training): 9.9807, loss (eval): 10.3366
Epoch: 23, loss (training): 9.9704, loss (eval): 10.3982
Epoch: 24, loss (training): 9.9517, loss (eval): 10.386
Epoch: 25, loss (training): 9.9608, loss (eval): 10.3545
Early-stopping. Training converged after 26 epochs.
start update posterior model
Epoch: 0, loss (training): 11.4842, loss (eval): 11.4919
Epoch: 1, loss (training): 11.4808, loss (eval): 11.4857
Epoch: 2, loss (training): 11.4848, loss (eval): 11.4778
Epoch: 3, loss (training): 11.4782, loss (eval): 11.4844
Epoch: 4, loss (training): 11.481, loss (eval): 11.4754
Epoch: 5, loss (training): 11.4788, loss (eval): 11.477
Epoch: 6, loss (training): 11.4769, loss (eval): 11.4807
Epoch: 7, loss (training): 11.4779, loss (eval): 11.481
Epoch: 8, loss (training): 11.4788, loss (eval): 11.4776
Epoch: 9, loss (training): 11.4803, loss (eval): 11.4713
Epoch: 10, loss (training): 11.4781, loss (eval): 11.4685
Epoch: 11, loss (training): 11.4749, loss (eval): 11.474
Epoch: 12, loss (training): 11.4784, loss (eval): 11.492
Epoch: 13, loss (training): 11.4786, loss (eval): 11.478
Epoch: 14, loss (training): 11.4814, loss (eval): 11.4802
Epoch: 15, loss (training): 11.4774, loss (eval): 11.4861
Epoch: 16, loss (training): 11.4796, loss (eval): 11.4866
Epoch: 17, loss (training): 11.4802, loss (eval): 11.4755
Epoch: 18, loss (training): 11.4784, loss (eval): 11.5072
Epoch: 19, loss (training): 11.4774, loss (eval): 11.4849
Epoch: 20, loss (training): 11.4766, loss (eval): 11.4766
Epoch: 21, loss (training): 11.477, loss (eval): 11.4816
Epoch: 22, loss (training): 11.48, loss (eval): 11.4894
Epoch: 23, loss (training): 11.4778, loss (eval): 11.4823
Epoch: 24, loss (training): 11.4758, loss (eval): 11.4792
Epoch: 25, loss (training): 11.478, loss (eval): 11.4729
Epoch: 26, loss (training): 11.4784, loss (eval): 11.4766
Epoch: 27, loss (training): 11.4801, loss (eval): 11.4789
Epoch: 28, loss (training): 11.4781, loss (eval): 11.4764
Epoch: 29, loss (training): 11.4788, loss (eval): 11.4806
Early-stopping. Training converged after 30 epochs.

Runtime:1348.46
0
1
2
3
4
5
6
7
8
9
