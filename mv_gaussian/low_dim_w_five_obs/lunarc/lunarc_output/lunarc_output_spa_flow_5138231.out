Input args:
Dim: 2
seed: 1
seed_data: 10
/home/samwiq/snpla/seq-posterior-approx-w-nf-dev/mv_gaussian/low_dim_w_five_obs/lunarc
/home/samwiq/snpla/seq-posterior-approx-w-nf-dev
[1.0, 0.6065306597126334, 0.36787944117144233, 0.22313016014842982, 0.1353352832366127, 0.0820849986238988, 0.049787068367863944, 0.0301973834223185, 0.01831563888873418, 0.011108996538242306]
start full training
Iteration: 1
optimizer_post_lr: [0.001]
prob_prior: 1.0
start update likelihood model
Epoch: 0, loss (training): 25.6391, loss (eval): 48.3265
Epoch: 1, loss (training): 19.7744, loss (eval): 21.5552
Epoch: 2, loss (training): 17.67, loss (eval): 18.3356
Epoch: 3, loss (training): 16.2847, loss (eval): 16.9781
Epoch: 4, loss (training): 15.0709, loss (eval): 15.6398
Epoch: 5, loss (training): 13.9663, loss (eval): 14.5085
Epoch: 6, loss (training): 13.1531, loss (eval): 13.5382
Epoch: 7, loss (training): 12.5468, loss (eval): 12.9305
Epoch: 8, loss (training): 11.974, loss (eval): 12.3654
Epoch: 9, loss (training): 11.5445, loss (eval): 11.7779
Epoch: 10, loss (training): 11.1489, loss (eval): 11.3469
Epoch: 11, loss (training): 10.8961, loss (eval): 11.2901
Epoch: 12, loss (training): 10.786, loss (eval): 10.8771
Epoch: 13, loss (training): 10.6102, loss (eval): 10.6804
Epoch: 14, loss (training): 10.4722, loss (eval): 10.6161
Epoch: 15, loss (training): 10.4724, loss (eval): 10.62
Epoch: 16, loss (training): 10.4944, loss (eval): 10.5726
Epoch: 17, loss (training): 10.3126, loss (eval): 10.4313
Epoch: 18, loss (training): 10.3583, loss (eval): 10.4917
Epoch: 19, loss (training): 10.3729, loss (eval): 10.563
Epoch: 20, loss (training): 10.3219, loss (eval): 10.4793
Epoch: 21, loss (training): 10.3145, loss (eval): 10.4737
Epoch: 22, loss (training): 10.3035, loss (eval): 10.4488
Epoch: 23, loss (training): 10.3075, loss (eval): 10.362
Epoch: 24, loss (training): 10.229, loss (eval): 10.3332
start update posterior model from prior pred - hot start
Epoch: 0, loss (training): 3.9186, loss (eval): 6.691
Epoch: 1, loss (training): 2.3654, loss (eval): 2.6334
Epoch: 2, loss (training): 1.6839, loss (eval): 2.2005
Epoch: 3, loss (training): 1.2136, loss (eval): 1.125
Epoch: 4, loss (training): 1.0876, loss (eval): 1.0092
Epoch: 5, loss (training): 0.8679, loss (eval): 1.0821
Epoch: 6, loss (training): 0.7187, loss (eval): 0.7327
Epoch: 7, loss (training): 0.7198, loss (eval): 0.6428
Epoch: 8, loss (training): 0.6184, loss (eval): 0.7031
Epoch: 9, loss (training): 0.5337, loss (eval): 0.5681
start update posterior model
Epoch: 0, loss (training): 11.2595, loss (eval): 11.3238
Epoch: 1, loss (training): 11.2495, loss (eval): 11.2126
Epoch: 2, loss (training): 11.2326, loss (eval): 11.2197
Epoch: 3, loss (training): 11.2378, loss (eval): 11.2632
Epoch: 4, loss (training): 11.232, loss (eval): 11.2124
Epoch: 5, loss (training): 11.2339, loss (eval): 11.2484
Epoch: 6, loss (training): 11.2361, loss (eval): 11.2413
Epoch: 7, loss (training): 11.2375, loss (eval): 11.2328
Epoch: 8, loss (training): 11.2364, loss (eval): 11.2322
Epoch: 9, loss (training): 11.23, loss (eval): 11.2175
Epoch: 10, loss (training): 11.2312, loss (eval): 11.2264
Epoch: 11, loss (training): 11.2398, loss (eval): 11.2559
Epoch: 12, loss (training): 11.2243, loss (eval): 11.2467
Epoch: 13, loss (training): 11.2331, loss (eval): 11.2214
Epoch: 14, loss (training): 11.23, loss (eval): 11.2088
Epoch: 15, loss (training): 11.2255, loss (eval): 11.2306
Epoch: 16, loss (training): 11.2301, loss (eval): 11.2271
Epoch: 17, loss (training): 11.2285, loss (eval): 11.2164
Epoch: 18, loss (training): 11.2329, loss (eval): 11.2252
Epoch: 19, loss (training): 11.2317, loss (eval): 11.2827
Epoch: 20, loss (training): 11.2283, loss (eval): 11.2455
Epoch: 21, loss (training): 11.2261, loss (eval): 11.2209
Epoch: 22, loss (training): 11.2271, loss (eval): 11.2146
Epoch: 23, loss (training): 11.2253, loss (eval): 11.2193
Epoch: 24, loss (training): 11.2269, loss (eval): 11.2255
Iteration: 2
optimizer_post_lr: [0.00099]
prob_prior: 0.6065306597126334
start update likelihood model
Epoch: 0, loss (training): 10.3856, loss (eval): 10.5756
Epoch: 1, loss (training): 10.2382, loss (eval): 10.5105
Epoch: 2, loss (training): 10.2113, loss (eval): 10.4122
Epoch: 3, loss (training): 10.2157, loss (eval): 10.41
Epoch: 4, loss (training): 10.1819, loss (eval): 10.5626
Epoch: 5, loss (training): 10.1869, loss (eval): 10.4766
Epoch: 6, loss (training): 10.1716, loss (eval): 10.5357
Epoch: 7, loss (training): 10.149, loss (eval): 10.3748
Epoch: 8, loss (training): 10.1836, loss (eval): 10.4948
Epoch: 9, loss (training): 10.1619, loss (eval): 10.4468
Epoch: 10, loss (training): 10.1835, loss (eval): 10.4282
Epoch: 11, loss (training): 10.1389, loss (eval): 10.5408
Epoch: 12, loss (training): 10.1177, loss (eval): 10.4634
Epoch: 13, loss (training): 10.1012, loss (eval): 10.3804
Epoch: 14, loss (training): 10.1251, loss (eval): 10.4958
Epoch: 15, loss (training): 10.128, loss (eval): 10.4443
Epoch: 16, loss (training): 10.1095, loss (eval): 10.4793
Epoch: 17, loss (training): 10.1021, loss (eval): 10.49
Epoch: 18, loss (training): 10.0813, loss (eval): 10.4526
Epoch: 19, loss (training): 10.0909, loss (eval): 10.6188
Epoch: 20, loss (training): 10.0985, loss (eval): 10.4676
Epoch: 21, loss (training): 10.0849, loss (eval): 10.4602
Epoch: 22, loss (training): 10.0627, loss (eval): 10.4213
Epoch: 23, loss (training): 10.069, loss (eval): 10.4209
Epoch: 24, loss (training): 10.0755, loss (eval): 10.5365
start update posterior model
Epoch: 0, loss (training): 11.3805, loss (eval): 11.7552
Epoch: 1, loss (training): 11.3674, loss (eval): 11.3493
Epoch: 2, loss (training): 11.3594, loss (eval): 11.3569
Epoch: 3, loss (training): 11.3614, loss (eval): 11.3615
Epoch: 4, loss (training): 11.3581, loss (eval): 11.3689
Epoch: 5, loss (training): 11.363, loss (eval): 11.3611
Epoch: 6, loss (training): 11.3568, loss (eval): 11.3457
Epoch: 7, loss (training): 11.3608, loss (eval): 11.3522
Epoch: 8, loss (training): 11.3595, loss (eval): 11.3561
Epoch: 9, loss (training): 11.359, loss (eval): 11.3591
Epoch: 10, loss (training): 11.3604, loss (eval): 11.3555
Epoch: 11, loss (training): 11.3605, loss (eval): 11.3578
Epoch: 12, loss (training): 11.3602, loss (eval): 11.3841
Epoch: 13, loss (training): 11.3621, loss (eval): 11.3635
Epoch: 14, loss (training): 11.3562, loss (eval): 11.3849
Epoch: 15, loss (training): 11.3604, loss (eval): 11.3673
Epoch: 16, loss (training): 11.3553, loss (eval): 11.3647
Epoch: 17, loss (training): 11.3593, loss (eval): 11.3473
Epoch: 18, loss (training): 11.3572, loss (eval): 11.3624
Epoch: 19, loss (training): 11.3564, loss (eval): 11.3471
Epoch: 20, loss (training): 11.3548, loss (eval): 11.3511
Epoch: 21, loss (training): 11.3575, loss (eval): 11.3464
Epoch: 22, loss (training): 11.3583, loss (eval): 11.3485
Epoch: 23, loss (training): 11.3585, loss (eval): 11.345
Epoch: 24, loss (training): 11.358, loss (eval): 11.3712
Iteration: 3
optimizer_post_lr: [0.0009801]
prob_prior: 0.36787944117144233
start update likelihood model
Epoch: 0, loss (training): 10.1924, loss (eval): 10.2891
Epoch: 1, loss (training): 10.1616, loss (eval): 10.3371
Epoch: 2, loss (training): 10.0943, loss (eval): 10.3011
Epoch: 3, loss (training): 10.0902, loss (eval): 10.215
Epoch: 4, loss (training): 10.1222, loss (eval): 10.4551
Epoch: 5, loss (training): 10.0801, loss (eval): 10.1962
Epoch: 6, loss (training): 10.0924, loss (eval): 10.2397
Epoch: 7, loss (training): 10.0904, loss (eval): 10.2679
Epoch: 8, loss (training): 10.0538, loss (eval): 10.2172
Epoch: 9, loss (training): 10.0519, loss (eval): 10.2787
Epoch: 10, loss (training): 10.0403, loss (eval): 10.2108
Epoch: 11, loss (training): 10.0534, loss (eval): 10.2409
Epoch: 12, loss (training): 10.0238, loss (eval): 10.2495
Epoch: 13, loss (training): 10.0508, loss (eval): 10.2593
Epoch: 14, loss (training): 10.0487, loss (eval): 10.3848
Epoch: 15, loss (training): 10.0398, loss (eval): 10.4189
Epoch: 16, loss (training): 10.024, loss (eval): 10.3341
Epoch: 17, loss (training): 10.0442, loss (eval): 10.2044
Epoch: 18, loss (training): 10.0358, loss (eval): 10.4116
Epoch: 19, loss (training): 10.058, loss (eval): 10.3291
Epoch: 20, loss (training): 10.0333, loss (eval): 10.2536
Epoch: 21, loss (training): 9.9912, loss (eval): 10.2256
Epoch: 22, loss (training): 10.0414, loss (eval): 10.2497
Epoch: 23, loss (training): 10.0381, loss (eval): 10.3733
Epoch: 24, loss (training): 10.0291, loss (eval): 10.4237
start update posterior model
Epoch: 0, loss (training): 11.5086, loss (eval): 11.5912
Epoch: 1, loss (training): 11.5085, loss (eval): 11.5019
Epoch: 2, loss (training): 11.5091, loss (eval): 11.5094
Epoch: 3, loss (training): 11.509, loss (eval): 11.5023
Epoch: 4, loss (training): 11.5088, loss (eval): 11.5017
Epoch: 5, loss (training): 11.505, loss (eval): 11.5018
Epoch: 6, loss (training): 11.5054, loss (eval): 11.5151
Epoch: 7, loss (training): 11.506, loss (eval): 11.4972
Epoch: 8, loss (training): 11.5084, loss (eval): 11.5026
Epoch: 9, loss (training): 11.5082, loss (eval): 11.5266
Epoch: 10, loss (training): 11.511, loss (eval): 11.494
Epoch: 11, loss (training): 11.5058, loss (eval): 11.4986
Epoch: 12, loss (training): 11.5053, loss (eval): 11.5078
Epoch: 13, loss (training): 11.5078, loss (eval): 11.4983
Epoch: 14, loss (training): 11.51, loss (eval): 11.4975
Epoch: 15, loss (training): 11.5058, loss (eval): 11.5292
Epoch: 16, loss (training): 11.5056, loss (eval): 11.5338
Epoch: 17, loss (training): 11.5065, loss (eval): 11.5079
Epoch: 18, loss (training): 11.5056, loss (eval): 11.5436
Epoch: 19, loss (training): 11.5101, loss (eval): 11.504
Epoch: 20, loss (training): 11.5051, loss (eval): 11.4978
Epoch: 21, loss (training): 11.5056, loss (eval): 11.5046
Epoch: 22, loss (training): 11.5066, loss (eval): 11.5106
Epoch: 23, loss (training): 11.5046, loss (eval): 11.5028
Epoch: 24, loss (training): 11.5061, loss (eval): 11.4982
Iteration: 4
optimizer_post_lr: [0.000970299]
prob_prior: 0.22313016014842982
start update likelihood model
Epoch: 0, loss (training): 10.1311, loss (eval): 10.3417
Epoch: 1, loss (training): 10.1208, loss (eval): 10.3754
Epoch: 2, loss (training): 10.0643, loss (eval): 10.4243
Epoch: 3, loss (training): 10.0391, loss (eval): 10.3827
Epoch: 4, loss (training): 10.039, loss (eval): 10.3906
Epoch: 5, loss (training): 10.0327, loss (eval): 10.3695
Epoch: 6, loss (training): 10.0245, loss (eval): 10.446
Epoch: 7, loss (training): 10.0375, loss (eval): 10.3341
Epoch: 8, loss (training): 10.021, loss (eval): 10.3881
Epoch: 9, loss (training): 10.0347, loss (eval): 10.3407
Epoch: 10, loss (training): 9.9863, loss (eval): 10.3555
Epoch: 11, loss (training): 10.001, loss (eval): 10.3605
Epoch: 12, loss (training): 10.0191, loss (eval): 10.4738
Epoch: 13, loss (training): 9.9985, loss (eval): 10.3577
Epoch: 14, loss (training): 9.9662, loss (eval): 10.4014
Epoch: 15, loss (training): 9.9857, loss (eval): 10.4433
Epoch: 16, loss (training): 10.0007, loss (eval): 10.372
Epoch: 17, loss (training): 9.9724, loss (eval): 10.393
Epoch: 18, loss (training): 9.984, loss (eval): 10.3606
Epoch: 19, loss (training): 9.9831, loss (eval): 10.394
Epoch: 20, loss (training): 9.9754, loss (eval): 10.4649
Epoch: 21, loss (training): 9.9781, loss (eval): 10.4471
Epoch: 22, loss (training): 9.9684, loss (eval): 10.3926
Epoch: 23, loss (training): 9.9745, loss (eval): 10.4645
Epoch: 24, loss (training): 10.0103, loss (eval): 10.4652
start update posterior model
Epoch: 0, loss (training): 11.193, loss (eval): 11.21
Epoch: 1, loss (training): 11.1963, loss (eval): 11.1892
Epoch: 2, loss (training): 11.1903, loss (eval): 11.1887
Epoch: 3, loss (training): 11.1925, loss (eval): 11.1848
Epoch: 4, loss (training): 11.1947, loss (eval): 11.1842
Epoch: 5, loss (training): 11.1927, loss (eval): 11.1879
Epoch: 6, loss (training): 11.1959, loss (eval): 11.1894
Epoch: 7, loss (training): 11.1932, loss (eval): 11.1869
Epoch: 8, loss (training): 11.1915, loss (eval): 11.1869
Epoch: 9, loss (training): 11.1916, loss (eval): 11.188
Epoch: 10, loss (training): 11.1903, loss (eval): 11.1829
Epoch: 11, loss (training): 11.1934, loss (eval): 11.193
Epoch: 12, loss (training): 11.1928, loss (eval): 11.1848
Epoch: 13, loss (training): 11.1937, loss (eval): 11.1989
Epoch: 14, loss (training): 11.1928, loss (eval): 11.1894
Epoch: 15, loss (training): 11.1886, loss (eval): 11.1881
Epoch: 16, loss (training): 11.1914, loss (eval): 11.1844
Epoch: 17, loss (training): 11.1923, loss (eval): 11.1896
Epoch: 18, loss (training): 11.1923, loss (eval): 11.1836
Epoch: 19, loss (training): 11.1911, loss (eval): 11.1888
Epoch: 20, loss (training): 11.1919, loss (eval): 11.1879
Epoch: 21, loss (training): 11.1914, loss (eval): 11.1919
Epoch: 22, loss (training): 11.1902, loss (eval): 11.1908
Epoch: 23, loss (training): 11.1936, loss (eval): 11.1886
Epoch: 24, loss (training): 11.192, loss (eval): 11.1841
Iteration: 5
optimizer_post_lr: [0.0009605960099999999]
prob_prior: 0.1353352832366127
start update likelihood model
Epoch: 0, loss (training): 10.2002, loss (eval): 10.177
Epoch: 1, loss (training): 10.1468, loss (eval): 10.1554
Epoch: 2, loss (training): 10.1164, loss (eval): 10.1864
Epoch: 3, loss (training): 10.099, loss (eval): 10.1402
Epoch: 4, loss (training): 10.0813, loss (eval): 10.2897
Epoch: 5, loss (training): 10.0823, loss (eval): 10.1578
Epoch: 6, loss (training): 10.1245, loss (eval): 10.1798
Epoch: 7, loss (training): 10.0583, loss (eval): 10.1976
Epoch: 8, loss (training): 10.0562, loss (eval): 10.1642
Epoch: 9, loss (training): 10.0466, loss (eval): 10.154
Epoch: 10, loss (training): 10.026, loss (eval): 10.1562
Epoch: 11, loss (training): 10.072, loss (eval): 10.172
Epoch: 12, loss (training): 10.0532, loss (eval): 10.1397
Epoch: 13, loss (training): 10.064, loss (eval): 10.1211
Epoch: 14, loss (training): 10.0471, loss (eval): 10.103
Epoch: 15, loss (training): 10.0596, loss (eval): 10.237
Epoch: 16, loss (training): 10.056, loss (eval): 10.2721
Epoch: 17, loss (training): 10.0526, loss (eval): 10.2096
Epoch: 18, loss (training): 10.0513, loss (eval): 10.193
Epoch: 19, loss (training): 10.0356, loss (eval): 10.1757
Epoch: 20, loss (training): 10.0404, loss (eval): 10.1789
Epoch: 21, loss (training): 10.0141, loss (eval): 10.1653
Epoch: 22, loss (training): 10.0254, loss (eval): 10.158
Epoch: 23, loss (training): 10.0234, loss (eval): 10.154
Epoch: 24, loss (training): 10.0214, loss (eval): 10.1851
start update posterior model
Epoch: 0, loss (training): 11.2858, loss (eval): 11.3214
Epoch: 1, loss (training): 11.2799, loss (eval): 11.2774
Epoch: 2, loss (training): 11.2827, loss (eval): 11.2772
Epoch: 3, loss (training): 11.2834, loss (eval): 11.2751
Epoch: 4, loss (training): 11.2813, loss (eval): 11.2778
Epoch: 5, loss (training): 11.281, loss (eval): 11.2764
Epoch: 6, loss (training): 11.2806, loss (eval): 11.3003
Epoch: 7, loss (training): 11.2833, loss (eval): 11.2854
Epoch: 8, loss (training): 11.2808, loss (eval): 11.2783
Epoch: 9, loss (training): 11.2826, loss (eval): 11.279
Epoch: 10, loss (training): 11.2808, loss (eval): 11.2755
Epoch: 11, loss (training): 11.282, loss (eval): 11.2736
Epoch: 12, loss (training): 11.2794, loss (eval): 11.2751
Epoch: 13, loss (training): 11.2816, loss (eval): 11.2947
Epoch: 14, loss (training): 11.2787, loss (eval): 11.2737
Epoch: 15, loss (training): 11.2819, loss (eval): 11.2714
Epoch: 16, loss (training): 11.2783, loss (eval): 11.2887
Epoch: 17, loss (training): 11.2841, loss (eval): 11.2807
Epoch: 18, loss (training): 11.2813, loss (eval): 11.2716
Epoch: 19, loss (training): 11.2818, loss (eval): 11.2769
Epoch: 20, loss (training): 11.2813, loss (eval): 11.2832
Epoch: 21, loss (training): 11.2836, loss (eval): 11.2752
Epoch: 22, loss (training): 11.2824, loss (eval): 11.276
Epoch: 23, loss (training): 11.2822, loss (eval): 11.2756
Epoch: 24, loss (training): 11.2787, loss (eval): 11.2806
Iteration: 6
optimizer_post_lr: [0.0009509900498999999]
prob_prior: 0.0820849986238988
start update likelihood model
Epoch: 0, loss (training): 10.1365, loss (eval): 10.0818
Epoch: 1, loss (training): 10.1065, loss (eval): 10.0931
Epoch: 2, loss (training): 10.0623, loss (eval): 10.0982
Epoch: 3, loss (training): 10.0414, loss (eval): 10.0056
Epoch: 4, loss (training): 10.0362, loss (eval): 10.0155
Epoch: 5, loss (training): 10.0341, loss (eval): 9.9885
Epoch: 6, loss (training): 10.0388, loss (eval): 10.0202
Epoch: 7, loss (training): 10.0266, loss (eval): 10.0216
Epoch: 8, loss (training): 10.0128, loss (eval): 10.0733
Epoch: 9, loss (training): 10.0137, loss (eval): 10.0984
Epoch: 10, loss (training): 9.9983, loss (eval): 10.05
Epoch: 11, loss (training): 10.0102, loss (eval): 10.1006
Epoch: 12, loss (training): 9.9925, loss (eval): 10.0627
Epoch: 13, loss (training): 9.9897, loss (eval): 10.0406
Epoch: 14, loss (training): 9.9954, loss (eval): 10.0337
Epoch: 15, loss (training): 9.9872, loss (eval): 10.1331
Epoch: 16, loss (training): 9.9901, loss (eval): 10.0947
Epoch: 17, loss (training): 9.9687, loss (eval): 10.1079
Epoch: 18, loss (training): 9.975, loss (eval): 10.0383
Epoch: 19, loss (training): 9.9793, loss (eval): 10.0494
Epoch: 20, loss (training): 9.9605, loss (eval): 10.0341
Epoch: 21, loss (training): 10.0006, loss (eval): 10.0647
Epoch: 22, loss (training): 9.9676, loss (eval): 10.0824
Epoch: 23, loss (training): 9.9629, loss (eval): 10.0684
Epoch: 24, loss (training): 9.9607, loss (eval): 10.0507
start update posterior model
Epoch: 0, loss (training): 11.3196, loss (eval): 11.3861
Epoch: 1, loss (training): 11.3201, loss (eval): 11.3423
Epoch: 2, loss (training): 11.3157, loss (eval): 11.3251
Epoch: 3, loss (training): 11.3174, loss (eval): 11.3191
Epoch: 4, loss (training): 11.3127, loss (eval): 11.3246
Epoch: 5, loss (training): 11.32, loss (eval): 11.3227
Epoch: 6, loss (training): 11.3189, loss (eval): 11.3202
Epoch: 7, loss (training): 11.3195, loss (eval): 11.3086
Epoch: 8, loss (training): 11.3169, loss (eval): 11.3238
Epoch: 9, loss (training): 11.313, loss (eval): 11.3179
Epoch: 10, loss (training): 11.3143, loss (eval): 11.3058
Epoch: 11, loss (training): 11.3164, loss (eval): 11.3139
Epoch: 12, loss (training): 11.3163, loss (eval): 11.3152
Epoch: 13, loss (training): 11.3169, loss (eval): 11.31
Epoch: 14, loss (training): 11.314, loss (eval): 11.3105
Epoch: 15, loss (training): 11.3159, loss (eval): 11.3105
Epoch: 16, loss (training): 11.3149, loss (eval): 11.3095
Epoch: 17, loss (training): 11.3153, loss (eval): 11.3135
Epoch: 18, loss (training): 11.3137, loss (eval): 11.3102
Epoch: 19, loss (training): 11.3146, loss (eval): 11.3073
Epoch: 20, loss (training): 11.3172, loss (eval): 11.307
Epoch: 21, loss (training): 11.3132, loss (eval): 11.3067
Epoch: 22, loss (training): 11.3139, loss (eval): 11.3114
Epoch: 23, loss (training): 11.3142, loss (eval): 11.304
Epoch: 24, loss (training): 11.3185, loss (eval): 11.3216
Iteration: 7
optimizer_post_lr: [0.0009414801494009999]
prob_prior: 0.049787068367863944
start update likelihood model
Epoch: 0, loss (training): 10.2615, loss (eval): 10.2606
Epoch: 1, loss (training): 10.1976, loss (eval): 10.1958
Epoch: 2, loss (training): 10.1729, loss (eval): 10.1505
Epoch: 3, loss (training): 10.1691, loss (eval): 10.2175
Epoch: 4, loss (training): 10.1389, loss (eval): 10.2205
Epoch: 5, loss (training): 10.122, loss (eval): 10.1788
Epoch: 6, loss (training): 10.1077, loss (eval): 10.1832
Epoch: 7, loss (training): 10.1194, loss (eval): 10.2015
Epoch: 8, loss (training): 10.1143, loss (eval): 10.1967
Epoch: 9, loss (training): 10.1552, loss (eval): 10.1706
Epoch: 10, loss (training): 10.1059, loss (eval): 10.1804
Epoch: 11, loss (training): 10.1198, loss (eval): 10.2171
Epoch: 12, loss (training): 10.0956, loss (eval): 10.1849
Epoch: 13, loss (training): 10.1258, loss (eval): 10.2355
Epoch: 14, loss (training): 10.0748, loss (eval): 10.2363
Epoch: 15, loss (training): 10.0668, loss (eval): 10.1778
Epoch: 16, loss (training): 10.0973, loss (eval): 10.215
Epoch: 17, loss (training): 10.0648, loss (eval): 10.2205
Epoch: 18, loss (training): 10.0618, loss (eval): 10.1342
Epoch: 19, loss (training): 10.0751, loss (eval): 10.27
Epoch: 20, loss (training): 10.0602, loss (eval): 10.2524
Epoch: 21, loss (training): 10.0493, loss (eval): 10.2254
Epoch: 22, loss (training): 10.0625, loss (eval): 10.2638
Epoch: 23, loss (training): 10.0652, loss (eval): 10.2836
Epoch: 24, loss (training): 10.0512, loss (eval): 10.2127
start update posterior model
Epoch: 0, loss (training): 11.2306, loss (eval): 11.2756
Epoch: 1, loss (training): 11.2276, loss (eval): 11.2368
Epoch: 2, loss (training): 11.226, loss (eval): 11.2224
Epoch: 3, loss (training): 11.2284, loss (eval): 11.2256
Epoch: 4, loss (training): 11.2282, loss (eval): 11.2242
Epoch: 5, loss (training): 11.2285, loss (eval): 11.2231
Epoch: 6, loss (training): 11.2281, loss (eval): 11.2296
Epoch: 7, loss (training): 11.2269, loss (eval): 11.2326
Epoch: 8, loss (training): 11.2279, loss (eval): 11.2239
Epoch: 9, loss (training): 11.2261, loss (eval): 11.2613
Epoch: 10, loss (training): 11.2268, loss (eval): 11.2282
Epoch: 11, loss (training): 11.2302, loss (eval): 11.2373
Epoch: 12, loss (training): 11.2265, loss (eval): 11.2228
Epoch: 13, loss (training): 11.2262, loss (eval): 11.2264
Epoch: 14, loss (training): 11.2286, loss (eval): 11.2236
Epoch: 15, loss (training): 11.229, loss (eval): 11.2281
Epoch: 16, loss (training): 11.2289, loss (eval): 11.2359
Epoch: 17, loss (training): 11.2264, loss (eval): 11.2282
Epoch: 18, loss (training): 11.2272, loss (eval): 11.2265
Epoch: 19, loss (training): 11.2286, loss (eval): 11.2273
Epoch: 20, loss (training): 11.2273, loss (eval): 11.2305
Epoch: 21, loss (training): 11.2281, loss (eval): 11.2221
Epoch: 22, loss (training): 11.2279, loss (eval): 11.2255
Epoch: 23, loss (training): 11.2283, loss (eval): 11.2326
Epoch: 24, loss (training): 11.2274, loss (eval): 11.2277
Iteration: 8
optimizer_post_lr: [0.0009320653479069899]
prob_prior: 0.0301973834223185
start update likelihood model
Epoch: 0, loss (training): 10.2028, loss (eval): 10.1982
Epoch: 1, loss (training): 10.1534, loss (eval): 10.1928
Epoch: 2, loss (training): 10.1502, loss (eval): 10.1942
Epoch: 3, loss (training): 10.1058, loss (eval): 10.185
Epoch: 4, loss (training): 10.1071, loss (eval): 10.2302
Epoch: 5, loss (training): 10.0864, loss (eval): 10.2187
Epoch: 6, loss (training): 10.1105, loss (eval): 10.3026
Epoch: 7, loss (training): 10.0869, loss (eval): 10.2589
Epoch: 8, loss (training): 10.0687, loss (eval): 10.1961
Epoch: 9, loss (training): 10.0795, loss (eval): 10.2335
Epoch: 10, loss (training): 10.0898, loss (eval): 10.2538
Epoch: 11, loss (training): 10.0685, loss (eval): 10.2078
Epoch: 12, loss (training): 10.0764, loss (eval): 10.2223
Epoch: 13, loss (training): 10.0549, loss (eval): 10.2331
Epoch: 14, loss (training): 10.0484, loss (eval): 10.2469
Epoch: 15, loss (training): 10.1035, loss (eval): 10.2345
Epoch: 16, loss (training): 10.0629, loss (eval): 10.2744
Epoch: 17, loss (training): 10.0516, loss (eval): 10.2197
Epoch: 18, loss (training): 10.0416, loss (eval): 10.2355
Epoch: 19, loss (training): 10.0893, loss (eval): 10.28
Epoch: 20, loss (training): 10.0635, loss (eval): 10.2516
Epoch: 21, loss (training): 10.0422, loss (eval): 10.2068
Epoch: 22, loss (training): 10.0592, loss (eval): 10.2719
Early-stopping. Training converged after 23 epochs.
start update posterior model
Epoch: 0, loss (training): 11.4238, loss (eval): 11.4596
Epoch: 1, loss (training): 11.4217, loss (eval): 11.4218
Epoch: 2, loss (training): 11.4219, loss (eval): 11.4369
Epoch: 3, loss (training): 11.4202, loss (eval): 11.4232
Epoch: 4, loss (training): 11.4203, loss (eval): 11.4186
Epoch: 5, loss (training): 11.421, loss (eval): 11.4183
Epoch: 6, loss (training): 11.4201, loss (eval): 11.4113
Epoch: 7, loss (training): 11.4206, loss (eval): 11.4449
Epoch: 8, loss (training): 11.4191, loss (eval): 11.4167
Epoch: 9, loss (training): 11.4192, loss (eval): 11.4171
Epoch: 10, loss (training): 11.418, loss (eval): 11.4184
Epoch: 11, loss (training): 11.4215, loss (eval): 11.4195
Epoch: 12, loss (training): 11.42, loss (eval): 11.4165
Epoch: 13, loss (training): 11.42, loss (eval): 11.4223
Epoch: 14, loss (training): 11.4209, loss (eval): 11.4257
Epoch: 15, loss (training): 11.4197, loss (eval): 11.4194
Epoch: 16, loss (training): 11.4199, loss (eval): 11.4119
Epoch: 17, loss (training): 11.4205, loss (eval): 11.4141
Epoch: 18, loss (training): 11.4217, loss (eval): 11.424
Epoch: 19, loss (training): 11.4199, loss (eval): 11.4156
Epoch: 20, loss (training): 11.42, loss (eval): 11.4313
Epoch: 21, loss (training): 11.4195, loss (eval): 11.4122
Epoch: 22, loss (training): 11.4201, loss (eval): 11.4339
Epoch: 23, loss (training): 11.4195, loss (eval): 11.4232
Epoch: 24, loss (training): 11.4193, loss (eval): 11.4189
Iteration: 9
optimizer_post_lr: [0.00092274469442792]
prob_prior: 0.01831563888873418
start update likelihood model
Epoch: 0, loss (training): 10.1591, loss (eval): 10.202
Epoch: 1, loss (training): 10.0983, loss (eval): 10.1446
Epoch: 2, loss (training): 10.0856, loss (eval): 10.1199
Epoch: 3, loss (training): 10.1019, loss (eval): 10.1778
Epoch: 4, loss (training): 10.0617, loss (eval): 10.152
Epoch: 5, loss (training): 10.0776, loss (eval): 10.1494
Epoch: 6, loss (training): 10.0609, loss (eval): 10.152
Epoch: 7, loss (training): 10.0435, loss (eval): 10.1537
Epoch: 8, loss (training): 10.0336, loss (eval): 10.1476
Epoch: 9, loss (training): 10.033, loss (eval): 10.232
Epoch: 10, loss (training): 10.0316, loss (eval): 10.1705
Epoch: 11, loss (training): 10.0384, loss (eval): 10.1343
Epoch: 12, loss (training): 10.0249, loss (eval): 10.1735
Epoch: 13, loss (training): 10.0195, loss (eval): 10.1991
Epoch: 14, loss (training): 10.007, loss (eval): 10.1791
Epoch: 15, loss (training): 10.0281, loss (eval): 10.1959
Epoch: 16, loss (training): 10.006, loss (eval): 10.1496
Epoch: 17, loss (training): 10.0173, loss (eval): 10.1472
Epoch: 18, loss (training): 9.9963, loss (eval): 10.1841
Epoch: 19, loss (training): 10.0093, loss (eval): 10.2233
Epoch: 20, loss (training): 10.0095, loss (eval): 10.2355
Epoch: 21, loss (training): 10.0103, loss (eval): 10.1797
Early-stopping. Training converged after 22 epochs.
start update posterior model
Epoch: 0, loss (training): 11.5091, loss (eval): 11.6353
Epoch: 1, loss (training): 11.5034, loss (eval): 11.5072
Epoch: 2, loss (training): 11.5019, loss (eval): 11.5063
Epoch: 3, loss (training): 11.5028, loss (eval): 11.5132
Epoch: 4, loss (training): 11.5022, loss (eval): 11.4994
Epoch: 5, loss (training): 11.5024, loss (eval): 11.4963
Epoch: 6, loss (training): 11.5011, loss (eval): 11.498
Epoch: 7, loss (training): 11.5034, loss (eval): 11.5015
Epoch: 8, loss (training): 11.5044, loss (eval): 11.5047
Epoch: 9, loss (training): 11.5034, loss (eval): 11.5036
Epoch: 10, loss (training): 11.5037, loss (eval): 11.501
Epoch: 11, loss (training): 11.5017, loss (eval): 11.4969
Epoch: 12, loss (training): 11.5027, loss (eval): 11.4923
Epoch: 13, loss (training): 11.5, loss (eval): 11.4975
Epoch: 14, loss (training): 11.5023, loss (eval): 11.4974
Epoch: 15, loss (training): 11.502, loss (eval): 11.4996
Epoch: 16, loss (training): 11.5018, loss (eval): 11.5031
Epoch: 17, loss (training): 11.501, loss (eval): 11.4996
Epoch: 18, loss (training): 11.5032, loss (eval): 11.5025
Epoch: 19, loss (training): 11.5024, loss (eval): 11.4951
Epoch: 20, loss (training): 11.5003, loss (eval): 11.495
Epoch: 21, loss (training): 11.504, loss (eval): 11.4974
Epoch: 22, loss (training): 11.5012, loss (eval): 11.5037
Epoch: 23, loss (training): 11.5032, loss (eval): 11.4942
Epoch: 24, loss (training): 11.5023, loss (eval): 11.5033
Iteration: 10
optimizer_post_lr: [0.0009135172474836408]
prob_prior: 0.011108996538242306
start update likelihood model
Epoch: 0, loss (training): 10.0786, loss (eval): 9.9713
Epoch: 1, loss (training): 10.0213, loss (eval): 9.9234
Epoch: 2, loss (training): 9.9922, loss (eval): 9.899
Epoch: 3, loss (training): 9.9791, loss (eval): 9.8659
Epoch: 4, loss (training): 9.9937, loss (eval): 9.8715
Epoch: 5, loss (training): 9.9583, loss (eval): 9.8616
Epoch: 6, loss (training): 9.9643, loss (eval): 9.912
Epoch: 7, loss (training): 9.968, loss (eval): 9.9478
Epoch: 8, loss (training): 9.9426, loss (eval): 9.872
Epoch: 9, loss (training): 9.9395, loss (eval): 9.8677
Epoch: 10, loss (training): 9.9346, loss (eval): 9.8843
Epoch: 11, loss (training): 9.9394, loss (eval): 9.9084
Epoch: 12, loss (training): 9.9442, loss (eval): 9.9155
Epoch: 13, loss (training): 9.9294, loss (eval): 9.8953
Epoch: 14, loss (training): 9.925, loss (eval): 9.936
Epoch: 15, loss (training): 9.9141, loss (eval): 9.913
Epoch: 16, loss (training): 9.9255, loss (eval): 9.8807
Epoch: 17, loss (training): 9.9222, loss (eval): 9.8684
Epoch: 18, loss (training): 9.9309, loss (eval): 9.8586
Epoch: 19, loss (training): 9.9089, loss (eval): 9.8927
Epoch: 20, loss (training): 9.9003, loss (eval): 9.9144
Epoch: 21, loss (training): 9.9107, loss (eval): 9.9167
Epoch: 22, loss (training): 9.9105, loss (eval): 9.9476
Epoch: 23, loss (training): 9.8982, loss (eval): 9.8537
Epoch: 24, loss (training): 9.908, loss (eval): 9.8914
start update posterior model
Epoch: 0, loss (training): 11.1647, loss (eval): 11.1952
Epoch: 1, loss (training): 11.1646, loss (eval): 11.1644
Epoch: 2, loss (training): 11.164, loss (eval): 11.1654
Epoch: 3, loss (training): 11.1626, loss (eval): 11.1585
Epoch: 4, loss (training): 11.1634, loss (eval): 11.1574
Epoch: 5, loss (training): 11.1635, loss (eval): 11.1657
Epoch: 6, loss (training): 11.1638, loss (eval): 11.1609
Epoch: 7, loss (training): 11.1659, loss (eval): 11.1803
Epoch: 8, loss (training): 11.1651, loss (eval): 11.1652
Epoch: 9, loss (training): 11.1634, loss (eval): 11.166
Epoch: 10, loss (training): 11.1652, loss (eval): 11.1677
Epoch: 11, loss (training): 11.1636, loss (eval): 11.1668
Epoch: 12, loss (training): 11.1644, loss (eval): 11.1623
Epoch: 13, loss (training): 11.1626, loss (eval): 11.1581
Epoch: 14, loss (training): 11.163, loss (eval): 11.1603
Epoch: 15, loss (training): 11.1647, loss (eval): 11.16
Epoch: 16, loss (training): 11.1631, loss (eval): 11.1603
Epoch: 17, loss (training): 11.1643, loss (eval): 11.1594
Epoch: 18, loss (training): 11.1635, loss (eval): 11.1635
Epoch: 19, loss (training): 11.1634, loss (eval): 11.1616
Epoch: 20, loss (training): 11.1609, loss (eval): 11.1583
Epoch: 21, loss (training): 11.1623, loss (eval): 11.1596
Epoch: 22, loss (training): 11.1645, loss (eval): 11.1639
Epoch: 23, loss (training): 11.162, loss (eval): 11.1836
Early-stopping. Training converged after 24 epochs.

Runtime:1211.87
0
1
2
3
4
5
6
7
8
9
