Input args:
Dim: 2
seed: 2
seed_data: 10
/home/samwiq/spa/seq-posterior-approx-w-nf-dev/mv_gaussian/low_dim_w_five_obs/lunarc
/home/samwiq/spa/seq-posterior-approx-w-nf-dev
[1.0, 0.44932896411722156, 0.20189651799465538, 0.09071795328941247, 0.04076220397836621, 0.01831563888873418, 0.008229747049020023, 0.003697863716482929, 0.001661557273173934, 0.0007465858083766792]
start full training
Iteration: 1
optimizer_post_lr: [0.001]
prob_prior: 1.0
start update likelihood model
Epoch: 0, loss (training): 26.858, loss (eval): 57.0604
Epoch: 1, loss (training): 20.3497, loss (eval): 22.3407
Epoch: 2, loss (training): 18.0086, loss (eval): 18.8113
Epoch: 3, loss (training): 16.6593, loss (eval): 17.2665
Epoch: 4, loss (training): 15.5837, loss (eval): 16.0882
Epoch: 5, loss (training): 14.5243, loss (eval): 15.0615
Epoch: 6, loss (training): 13.563, loss (eval): 14.0528
Epoch: 7, loss (training): 12.8108, loss (eval): 13.3823
Epoch: 8, loss (training): 12.0153, loss (eval): 12.5017
Epoch: 9, loss (training): 11.3866, loss (eval): 11.8427
Epoch: 10, loss (training): 11.1168, loss (eval): 11.6598
Epoch: 11, loss (training): 10.8427, loss (eval): 11.0178
Epoch: 12, loss (training): 10.7305, loss (eval): 10.8835
Epoch: 13, loss (training): 10.6872, loss (eval): 11.255
Epoch: 14, loss (training): 10.4869, loss (eval): 10.6652
Epoch: 15, loss (training): 10.5156, loss (eval): 10.5831
Epoch: 16, loss (training): 10.5059, loss (eval): 10.6995
Epoch: 17, loss (training): 10.3086, loss (eval): 10.6455
Epoch: 18, loss (training): 10.2971, loss (eval): 10.3338
Epoch: 19, loss (training): 10.257, loss (eval): 10.4893
Epoch: 20, loss (training): 10.2787, loss (eval): 10.3025
Epoch: 21, loss (training): 10.22, loss (eval): 10.4241
Epoch: 22, loss (training): 10.2756, loss (eval): 10.5179
Epoch: 23, loss (training): 10.1933, loss (eval): 10.2954
Epoch: 24, loss (training): 10.1791, loss (eval): 10.2981
start update posterior model from prior pred - hot start
Epoch: 0, loss (training): 3.6847, loss (eval): 7.207
Epoch: 1, loss (training): 2.1919, loss (eval): 2.4853
Epoch: 2, loss (training): 1.4541, loss (eval): 1.7865
Epoch: 3, loss (training): 1.1467, loss (eval): 1.319
Epoch: 4, loss (training): 0.9985, loss (eval): 1.3273
Epoch: 5, loss (training): 0.8053, loss (eval): 0.9035
Epoch: 6, loss (training): 0.724, loss (eval): 1.0047
Epoch: 7, loss (training): 0.7667, loss (eval): 0.8822
Epoch: 8, loss (training): 0.5677, loss (eval): 0.8419
Epoch: 9, loss (training): 0.69, loss (eval): 0.6322
start update posterior model
Epoch: 0, loss (training): 10.7729, loss (eval): 11.098
Epoch: 1, loss (training): 10.7643, loss (eval): 10.765
Epoch: 2, loss (training): 10.7564, loss (eval): 10.7809
Epoch: 3, loss (training): 10.7572, loss (eval): 10.7379
Epoch: 4, loss (training): 10.761, loss (eval): 10.7414
Epoch: 5, loss (training): 10.7836, loss (eval): 10.7468
Epoch: 6, loss (training): 10.7626, loss (eval): 10.7427
Epoch: 7, loss (training): 10.7634, loss (eval): 10.7516
Epoch: 8, loss (training): 10.7618, loss (eval): 10.74
Epoch: 9, loss (training): 10.7538, loss (eval): 10.7487
Epoch: 10, loss (training): 10.7555, loss (eval): 10.7381
Epoch: 11, loss (training): 10.7579, loss (eval): 10.7621
Epoch: 12, loss (training): 10.7575, loss (eval): 10.7502
Epoch: 13, loss (training): 10.7512, loss (eval): 10.7691
Epoch: 14, loss (training): 10.7549, loss (eval): 10.7603
Epoch: 15, loss (training): 10.7573, loss (eval): 10.742
Epoch: 16, loss (training): 10.7618, loss (eval): 10.7367
Epoch: 17, loss (training): 10.7487, loss (eval): 10.7393
Epoch: 18, loss (training): 10.7549, loss (eval): 10.7507
Epoch: 19, loss (training): 10.7534, loss (eval): 10.7412
Epoch: 20, loss (training): 10.7538, loss (eval): 10.7331
Epoch: 21, loss (training): 10.7533, loss (eval): 10.7363
Epoch: 22, loss (training): 10.7553, loss (eval): 10.7525
Epoch: 23, loss (training): 10.7517, loss (eval): 10.7731
Epoch: 24, loss (training): 10.7492, loss (eval): 10.7502
Iteration: 2
optimizer_post_lr: [0.001]
prob_prior: 0.44932896411722156
start update likelihood model
Epoch: 0, loss (training): 10.2741, loss (eval): 10.1858
Epoch: 1, loss (training): 10.2058, loss (eval): 10.1798
Epoch: 2, loss (training): 10.2329, loss (eval): 10.1978
Epoch: 3, loss (training): 10.1893, loss (eval): 10.1136
Epoch: 4, loss (training): 10.1882, loss (eval): 10.0683
Epoch: 5, loss (training): 10.1784, loss (eval): 10.1392
Epoch: 6, loss (training): 10.1607, loss (eval): 10.1454
Epoch: 7, loss (training): 10.1325, loss (eval): 10.2768
Epoch: 8, loss (training): 10.1338, loss (eval): 10.1984
Epoch: 9, loss (training): 10.1456, loss (eval): 10.2369
Epoch: 10, loss (training): 10.1025, loss (eval): 10.1417
Epoch: 11, loss (training): 10.1345, loss (eval): 10.1246
Epoch: 12, loss (training): 10.1214, loss (eval): 10.1649
Epoch: 13, loss (training): 10.1174, loss (eval): 10.1075
Epoch: 14, loss (training): 10.1134, loss (eval): 10.0839
Epoch: 15, loss (training): 10.1292, loss (eval): 10.0711
Epoch: 16, loss (training): 10.1139, loss (eval): 10.1309
Epoch: 17, loss (training): 10.0988, loss (eval): 10.2298
Epoch: 18, loss (training): 10.0942, loss (eval): 10.1514
Epoch: 19, loss (training): 10.0602, loss (eval): 10.1065
Epoch: 20, loss (training): 10.0806, loss (eval): 10.1259
Epoch: 21, loss (training): 10.0835, loss (eval): 10.029
Epoch: 22, loss (training): 10.0787, loss (eval): 10.1457
Epoch: 23, loss (training): 10.0805, loss (eval): 10.1749
Epoch: 24, loss (training): 10.1022, loss (eval): 10.1653
start update posterior model
Epoch: 0, loss (training): 10.9922, loss (eval): 11.0014
Epoch: 1, loss (training): 10.9922, loss (eval): 10.9887
Epoch: 2, loss (training): 10.9904, loss (eval): 10.9811
Epoch: 3, loss (training): 11.0006, loss (eval): 10.9862
Epoch: 4, loss (training): 10.9903, loss (eval): 11.0035
Epoch: 5, loss (training): 10.9961, loss (eval): 10.9792
Epoch: 6, loss (training): 10.9892, loss (eval): 10.9913
Epoch: 7, loss (training): 10.9889, loss (eval): 10.9992
Epoch: 8, loss (training): 10.9886, loss (eval): 10.9842
Epoch: 9, loss (training): 10.9866, loss (eval): 10.9771
Epoch: 10, loss (training): 10.9928, loss (eval): 10.9881
Epoch: 11, loss (training): 10.9897, loss (eval): 11.0426
Epoch: 12, loss (training): 10.9845, loss (eval): 10.9834
Epoch: 13, loss (training): 10.9894, loss (eval): 10.9837
Epoch: 14, loss (training): 10.9908, loss (eval): 10.9984
Epoch: 15, loss (training): 10.9867, loss (eval): 10.9741
Epoch: 16, loss (training): 10.9899, loss (eval): 10.9775
Epoch: 17, loss (training): 10.9885, loss (eval): 10.9809
Epoch: 18, loss (training): 10.9887, loss (eval): 11.0287
Epoch: 19, loss (training): 10.9886, loss (eval): 10.9763
Epoch: 20, loss (training): 10.9872, loss (eval): 10.9811
Epoch: 21, loss (training): 10.9854, loss (eval): 10.9937
Epoch: 22, loss (training): 10.9896, loss (eval): 10.9882
Epoch: 23, loss (training): 10.9898, loss (eval): 10.9849
Epoch: 24, loss (training): 10.9926, loss (eval): 11.0137
Iteration: 3
optimizer_post_lr: [0.001]
prob_prior: 0.20189651799465538
start update likelihood model
Epoch: 0, loss (training): 10.2936, loss (eval): 10.1392
Epoch: 1, loss (training): 10.2553, loss (eval): 10.2162
Epoch: 2, loss (training): 10.1931, loss (eval): 10.158
Epoch: 3, loss (training): 10.1781, loss (eval): 10.147
Epoch: 4, loss (training): 10.1655, loss (eval): 10.1391
Epoch: 5, loss (training): 10.1916, loss (eval): 10.1083
Epoch: 6, loss (training): 10.1403, loss (eval): 10.2121
Epoch: 7, loss (training): 10.1464, loss (eval): 10.1038
Epoch: 8, loss (training): 10.1366, loss (eval): 10.0891
Epoch: 9, loss (training): 10.1436, loss (eval): 10.1136
Epoch: 10, loss (training): 10.1154, loss (eval): 10.2635
Epoch: 11, loss (training): 10.1281, loss (eval): 10.0006
Epoch: 12, loss (training): 10.1213, loss (eval): 10.1724
Epoch: 13, loss (training): 10.1052, loss (eval): 10.138
Epoch: 14, loss (training): 10.0885, loss (eval): 10.0873
Epoch: 15, loss (training): 10.1122, loss (eval): 10.1286
Epoch: 16, loss (training): 10.1014, loss (eval): 10.1681
Epoch: 17, loss (training): 10.0876, loss (eval): 10.058
Epoch: 18, loss (training): 10.0899, loss (eval): 10.0492
Epoch: 19, loss (training): 10.0964, loss (eval): 10.1203
Epoch: 20, loss (training): 10.0981, loss (eval): 10.1476
Epoch: 21, loss (training): 10.1098, loss (eval): 10.1973
Epoch: 22, loss (training): 10.0922, loss (eval): 10.1676
Epoch: 23, loss (training): 10.0718, loss (eval): 10.183
Epoch: 24, loss (training): 10.0426, loss (eval): 10.1346
start update posterior model
Epoch: 0, loss (training): 11.2914, loss (eval): 11.3033
Epoch: 1, loss (training): 11.2855, loss (eval): 11.2841
Epoch: 2, loss (training): 11.2864, loss (eval): 11.2807
Epoch: 3, loss (training): 11.2846, loss (eval): 11.2819
Epoch: 4, loss (training): 11.2864, loss (eval): 11.2957
Epoch: 5, loss (training): 11.291, loss (eval): 11.2804
Epoch: 6, loss (training): 11.2908, loss (eval): 11.2727
Epoch: 7, loss (training): 11.2846, loss (eval): 11.2864
Epoch: 8, loss (training): 11.2853, loss (eval): 11.2817
Epoch: 9, loss (training): 11.2823, loss (eval): 11.2787
Epoch: 10, loss (training): 11.2855, loss (eval): 11.3087
Epoch: 11, loss (training): 11.2858, loss (eval): 11.2983
Epoch: 12, loss (training): 11.2889, loss (eval): 11.3359
Epoch: 13, loss (training): 11.2845, loss (eval): 11.2775
Epoch: 14, loss (training): 11.2865, loss (eval): 11.2893
Epoch: 15, loss (training): 11.2905, loss (eval): 11.2885
Epoch: 16, loss (training): 11.2814, loss (eval): 11.2836
Epoch: 17, loss (training): 11.2873, loss (eval): 11.2965
Epoch: 18, loss (training): 11.2865, loss (eval): 11.2883
Epoch: 19, loss (training): 11.2863, loss (eval): 11.2907
Epoch: 20, loss (training): 11.2848, loss (eval): 11.2853
Epoch: 21, loss (training): 11.2854, loss (eval): 11.3197
Epoch: 22, loss (training): 11.2847, loss (eval): 11.2778
Epoch: 23, loss (training): 11.2844, loss (eval): 11.2837
Epoch: 24, loss (training): 11.2845, loss (eval): 11.2775
Iteration: 4
optimizer_post_lr: [0.001]
prob_prior: 0.09071795328941247
start update likelihood model
Epoch: 0, loss (training): 10.2422, loss (eval): 10.2037
Epoch: 1, loss (training): 10.1916, loss (eval): 10.1606
Epoch: 2, loss (training): 10.1779, loss (eval): 10.1789
Epoch: 3, loss (training): 10.163, loss (eval): 10.1349
Epoch: 4, loss (training): 10.1482, loss (eval): 10.1585
Epoch: 5, loss (training): 10.1642, loss (eval): 10.1582
Epoch: 6, loss (training): 10.1408, loss (eval): 10.2072
Epoch: 7, loss (training): 10.1278, loss (eval): 10.1893
Epoch: 8, loss (training): 10.1523, loss (eval): 10.2301
Epoch: 9, loss (training): 10.1393, loss (eval): 10.1059
Epoch: 10, loss (training): 10.1292, loss (eval): 10.2191
Epoch: 11, loss (training): 10.1657, loss (eval): 10.1842
Epoch: 12, loss (training): 10.1675, loss (eval): 10.2455
Epoch: 13, loss (training): 10.1462, loss (eval): 10.1863
Epoch: 14, loss (training): 10.1336, loss (eval): 10.2154
Epoch: 15, loss (training): 10.1349, loss (eval): 10.2795
Epoch: 16, loss (training): 10.1222, loss (eval): 10.2228
Epoch: 17, loss (training): 10.1319, loss (eval): 10.1396
Epoch: 18, loss (training): 10.1011, loss (eval): 10.1377
Epoch: 19, loss (training): 10.0959, loss (eval): 10.1822
Epoch: 20, loss (training): 10.1196, loss (eval): 10.2543
Epoch: 21, loss (training): 10.1066, loss (eval): 10.2339
Epoch: 22, loss (training): 10.1132, loss (eval): 10.2075
Epoch: 23, loss (training): 10.1092, loss (eval): 10.2654
Epoch: 24, loss (training): 10.1119, loss (eval): 10.2188
start update posterior model
Epoch: 0, loss (training): 11.2564, loss (eval): 11.346
Epoch: 1, loss (training): 11.2571, loss (eval): 11.2443
Epoch: 2, loss (training): 11.2522, loss (eval): 11.2432
Epoch: 3, loss (training): 11.2508, loss (eval): 11.248
Epoch: 4, loss (training): 11.2513, loss (eval): 11.2735
Epoch: 5, loss (training): 11.2477, loss (eval): 11.2447
Epoch: 6, loss (training): 11.2512, loss (eval): 11.2431
Epoch: 7, loss (training): 11.2548, loss (eval): 11.2531
Epoch: 8, loss (training): 11.2538, loss (eval): 11.2697
Epoch: 9, loss (training): 11.2499, loss (eval): 11.2405
Epoch: 10, loss (training): 11.2476, loss (eval): 11.2492
Epoch: 11, loss (training): 11.2489, loss (eval): 11.2452
Epoch: 12, loss (training): 11.2558, loss (eval): 11.2446
Epoch: 13, loss (training): 11.2527, loss (eval): 11.2464
Epoch: 14, loss (training): 11.251, loss (eval): 11.2474
Epoch: 15, loss (training): 11.2541, loss (eval): 11.2528
Epoch: 16, loss (training): 11.2516, loss (eval): 11.2605
Epoch: 17, loss (training): 11.2522, loss (eval): 11.2632
Epoch: 18, loss (training): 11.2504, loss (eval): 11.2455
Epoch: 19, loss (training): 11.2514, loss (eval): 11.2517
Epoch: 20, loss (training): 11.2517, loss (eval): 11.2404
Epoch: 21, loss (training): 11.253, loss (eval): 11.2668
Epoch: 22, loss (training): 11.2502, loss (eval): 11.2563
Epoch: 23, loss (training): 11.2499, loss (eval): 11.2681
Epoch: 24, loss (training): 11.2509, loss (eval): 11.2412
Iteration: 5
optimizer_post_lr: [0.001]
prob_prior: 0.04076220397836621
start update likelihood model
Epoch: 0, loss (training): 10.2315, loss (eval): 10.1707
Epoch: 1, loss (training): 10.1653, loss (eval): 10.2058
Epoch: 2, loss (training): 10.1312, loss (eval): 10.1936
Epoch: 3, loss (training): 10.1392, loss (eval): 10.2456
Epoch: 4, loss (training): 10.1477, loss (eval): 10.1899
Epoch: 5, loss (training): 10.1299, loss (eval): 10.1871
Epoch: 6, loss (training): 10.108, loss (eval): 10.2223
Epoch: 7, loss (training): 10.0929, loss (eval): 10.1622
Epoch: 8, loss (training): 10.1142, loss (eval): 10.199
Epoch: 9, loss (training): 10.1246, loss (eval): 10.2374
Epoch: 10, loss (training): 10.0876, loss (eval): 10.159
Epoch: 11, loss (training): 10.1041, loss (eval): 10.2321
Epoch: 12, loss (training): 10.1419, loss (eval): 10.3383
Epoch: 13, loss (training): 10.1228, loss (eval): 10.2007
Epoch: 14, loss (training): 10.115, loss (eval): 10.2312
Epoch: 15, loss (training): 10.1268, loss (eval): 10.2064
Epoch: 16, loss (training): 10.0818, loss (eval): 10.176
Epoch: 17, loss (training): 10.06, loss (eval): 10.1549
Epoch: 18, loss (training): 10.1011, loss (eval): 10.2581
Epoch: 19, loss (training): 10.1175, loss (eval): 10.1682
Epoch: 20, loss (training): 10.0639, loss (eval): 10.169
Epoch: 21, loss (training): 10.0801, loss (eval): 10.1981
Epoch: 22, loss (training): 10.0633, loss (eval): 10.2228
Epoch: 23, loss (training): 10.0633, loss (eval): 10.246
Epoch: 24, loss (training): 10.1062, loss (eval): 10.1478
start update posterior model
Epoch: 0, loss (training): 11.0707, loss (eval): 11.0845
Epoch: 1, loss (training): 11.0725, loss (eval): 11.0805
Epoch: 2, loss (training): 11.0733, loss (eval): 11.066
Epoch: 3, loss (training): 11.0707, loss (eval): 11.0745
Epoch: 4, loss (training): 11.0706, loss (eval): 11.0957
Epoch: 5, loss (training): 11.0696, loss (eval): 11.0654
Epoch: 6, loss (training): 11.0707, loss (eval): 11.0685
Epoch: 7, loss (training): 11.0741, loss (eval): 11.0627
Epoch: 8, loss (training): 11.0739, loss (eval): 11.084
Epoch: 9, loss (training): 11.0718, loss (eval): 11.0777
Epoch: 10, loss (training): 11.0686, loss (eval): 11.0898
Epoch: 11, loss (training): 11.0729, loss (eval): 11.0699
Epoch: 12, loss (training): 11.0726, loss (eval): 11.0631
Epoch: 13, loss (training): 11.0716, loss (eval): 11.0881
Epoch: 14, loss (training): 11.0705, loss (eval): 11.0676
Epoch: 15, loss (training): 11.0724, loss (eval): 11.103
Epoch: 16, loss (training): 11.0746, loss (eval): 11.0772
Epoch: 17, loss (training): 11.0709, loss (eval): 11.0765
Epoch: 18, loss (training): 11.072, loss (eval): 11.0605
Epoch: 19, loss (training): 11.0682, loss (eval): 11.0693
Epoch: 20, loss (training): 11.0711, loss (eval): 11.0603
Epoch: 21, loss (training): 11.0721, loss (eval): 11.0702
Epoch: 22, loss (training): 11.0677, loss (eval): 11.0861
Epoch: 23, loss (training): 11.073, loss (eval): 11.0612
Epoch: 24, loss (training): 11.0731, loss (eval): 11.0676
Iteration: 6
optimizer_post_lr: [0.001]
prob_prior: 0.01831563888873418
start update likelihood model
Epoch: 0, loss (training): 10.1426, loss (eval): 10.1094
Epoch: 1, loss (training): 10.0709, loss (eval): 10.1314
Epoch: 2, loss (training): 10.0751, loss (eval): 10.169
Epoch: 3, loss (training): 10.0585, loss (eval): 10.1151
Epoch: 4, loss (training): 10.0534, loss (eval): 10.1337
Epoch: 5, loss (training): 10.0434, loss (eval): 10.1159
Epoch: 6, loss (training): 10.0455, loss (eval): 10.1011
Epoch: 7, loss (training): 10.0284, loss (eval): 10.0428
Epoch: 8, loss (training): 10.0361, loss (eval): 10.1074
Epoch: 9, loss (training): 10.0194, loss (eval): 10.1323
Epoch: 10, loss (training): 10.0281, loss (eval): 10.0709
Epoch: 11, loss (training): 10.0091, loss (eval): 10.1042
Epoch: 12, loss (training): 10.0185, loss (eval): 10.0933
Epoch: 13, loss (training): 10.0038, loss (eval): 10.1203
Epoch: 14, loss (training): 10.021, loss (eval): 10.1386
Epoch: 15, loss (training): 10.002, loss (eval): 10.1151
Epoch: 16, loss (training): 10.0147, loss (eval): 10.1096
Epoch: 17, loss (training): 9.9901, loss (eval): 10.113
Epoch: 18, loss (training): 9.9884, loss (eval): 10.0758
Epoch: 19, loss (training): 10.0097, loss (eval): 10.0675
Epoch: 20, loss (training): 10.0284, loss (eval): 10.2234
Epoch: 21, loss (training): 9.998, loss (eval): 10.2168
Epoch: 22, loss (training): 9.9815, loss (eval): 10.1104
Epoch: 23, loss (training): 9.9936, loss (eval): 10.1588
Epoch: 24, loss (training): 9.9942, loss (eval): 10.0952
start update posterior model
Epoch: 0, loss (training): 10.9806, loss (eval): 10.9964
Epoch: 1, loss (training): 10.9812, loss (eval): 10.9782
Epoch: 2, loss (training): 10.9845, loss (eval): 10.9785
Epoch: 3, loss (training): 10.9783, loss (eval): 10.9736
Epoch: 4, loss (training): 10.9808, loss (eval): 10.977
Epoch: 5, loss (training): 10.9852, loss (eval): 10.9749
Epoch: 6, loss (training): 10.9827, loss (eval): 10.9821
Epoch: 7, loss (training): 10.9837, loss (eval): 10.9793
Epoch: 8, loss (training): 10.9783, loss (eval): 10.9712
Epoch: 9, loss (training): 10.9813, loss (eval): 10.9812
Epoch: 10, loss (training): 10.9836, loss (eval): 10.9806
Epoch: 11, loss (training): 10.9849, loss (eval): 10.9814
Epoch: 12, loss (training): 10.9839, loss (eval): 10.9712
Epoch: 13, loss (training): 10.9841, loss (eval): 10.9797
Epoch: 14, loss (training): 10.9816, loss (eval): 10.97
Epoch: 15, loss (training): 10.9809, loss (eval): 10.9798
Epoch: 16, loss (training): 10.9841, loss (eval): 10.9741
Epoch: 17, loss (training): 10.9808, loss (eval): 10.9845
Epoch: 18, loss (training): 10.9786, loss (eval): 10.9823
Epoch: 19, loss (training): 10.9805, loss (eval): 10.9922
Epoch: 20, loss (training): 10.9851, loss (eval): 10.9909
Epoch: 21, loss (training): 10.9824, loss (eval): 10.9764
Epoch: 22, loss (training): 10.9792, loss (eval): 10.9826
Epoch: 23, loss (training): 10.981, loss (eval): 10.9782
Epoch: 24, loss (training): 10.9826, loss (eval): 10.983
Iteration: 7
optimizer_post_lr: [0.001]
prob_prior: 0.008229747049020023
start update likelihood model
Epoch: 0, loss (training): 10.2322, loss (eval): 10.2577
Epoch: 1, loss (training): 10.1482, loss (eval): 10.2244
Epoch: 2, loss (training): 10.1428, loss (eval): 10.2208
Epoch: 3, loss (training): 10.1532, loss (eval): 10.2329
Epoch: 4, loss (training): 10.1341, loss (eval): 10.264
Epoch: 5, loss (training): 10.1376, loss (eval): 10.3022
Epoch: 6, loss (training): 10.1339, loss (eval): 10.3855
Epoch: 7, loss (training): 10.0939, loss (eval): 10.2624
Epoch: 8, loss (training): 10.1058, loss (eval): 10.2541
Epoch: 9, loss (training): 10.1214, loss (eval): 10.2924
Epoch: 10, loss (training): 10.1122, loss (eval): 10.2975
Epoch: 11, loss (training): 10.0878, loss (eval): 10.311
Epoch: 12, loss (training): 10.1011, loss (eval): 10.3608
Epoch: 13, loss (training): 10.0885, loss (eval): 10.3325
Epoch: 14, loss (training): 10.0836, loss (eval): 10.4024
Epoch: 15, loss (training): 10.0838, loss (eval): 10.3033
Epoch: 16, loss (training): 10.0667, loss (eval): 10.3506
Epoch: 17, loss (training): 10.0851, loss (eval): 10.2863
Epoch: 18, loss (training): 10.0857, loss (eval): 10.2997
Epoch: 19, loss (training): 10.0778, loss (eval): 10.3515
Epoch: 20, loss (training): 10.0709, loss (eval): 10.3738
Epoch: 21, loss (training): 10.0676, loss (eval): 10.391
Early-stopping. Training converged after 22 epochs.
start update posterior model
Epoch: 0, loss (training): 11.0048, loss (eval): 11.039
Epoch: 1, loss (training): 11.0047, loss (eval): 11.0064
Epoch: 2, loss (training): 11.0038, loss (eval): 11.0089
Epoch: 3, loss (training): 11.0048, loss (eval): 11.0081
Epoch: 4, loss (training): 11.003, loss (eval): 11.0069
Epoch: 5, loss (training): 11.0021, loss (eval): 10.9949
Epoch: 6, loss (training): 11.0021, loss (eval): 10.9986
Epoch: 7, loss (training): 11.0036, loss (eval): 10.9965
Epoch: 8, loss (training): 11.0044, loss (eval): 11.0124
Epoch: 9, loss (training): 11.0012, loss (eval): 11.0183
Epoch: 10, loss (training): 11.0019, loss (eval): 11.0093
Epoch: 11, loss (training): 11.0017, loss (eval): 10.997
Epoch: 12, loss (training): 11.0056, loss (eval): 11.011
Epoch: 13, loss (training): 11.0011, loss (eval): 10.9983
Epoch: 14, loss (training): 11.0021, loss (eval): 11.0097
Epoch: 15, loss (training): 11.0028, loss (eval): 10.9955
Epoch: 16, loss (training): 11.001, loss (eval): 10.9993
Epoch: 17, loss (training): 11.0021, loss (eval): 10.997
Epoch: 18, loss (training): 11.0033, loss (eval): 10.9972
Epoch: 19, loss (training): 11.0038, loss (eval): 11.0072
Epoch: 20, loss (training): 11.0022, loss (eval): 11.0062
Epoch: 21, loss (training): 11.0044, loss (eval): 11.0097
Epoch: 22, loss (training): 11.0026, loss (eval): 10.9962
Epoch: 23, loss (training): 11.0005, loss (eval): 10.9994
Epoch: 24, loss (training): 11.0025, loss (eval): 11.004
Iteration: 8
optimizer_post_lr: [0.001]
prob_prior: 0.003697863716482929
start update likelihood model
Epoch: 0, loss (training): 10.2477, loss (eval): 10.006
Epoch: 1, loss (training): 10.2095, loss (eval): 10.0302
Epoch: 2, loss (training): 10.1531, loss (eval): 10.087
Epoch: 3, loss (training): 10.1385, loss (eval): 10.0275
Epoch: 4, loss (training): 10.1087, loss (eval): 10.0146
Epoch: 5, loss (training): 10.1206, loss (eval): 10.0668
Epoch: 6, loss (training): 10.1119, loss (eval): 10.0435
Epoch: 7, loss (training): 10.1167, loss (eval): 10.0376
Epoch: 8, loss (training): 10.099, loss (eval): 10.0365
Epoch: 9, loss (training): 10.1027, loss (eval): 10.0424
Epoch: 10, loss (training): 10.1041, loss (eval): 10.0976
Epoch: 11, loss (training): 10.0913, loss (eval): 10.0079
Epoch: 12, loss (training): 10.0995, loss (eval): 10.0667
Epoch: 13, loss (training): 10.0821, loss (eval): 10.0131
Epoch: 14, loss (training): 10.0835, loss (eval): 10.0187
Epoch: 15, loss (training): 10.0675, loss (eval): 10.0219
Epoch: 16, loss (training): 10.1044, loss (eval): 10.0299
Epoch: 17, loss (training): 10.0909, loss (eval): 10.0653
Epoch: 18, loss (training): 10.072, loss (eval): 10.1426
Epoch: 19, loss (training): 10.0709, loss (eval): 10.112
Early-stopping. Training converged after 20 epochs.
start update posterior model
Epoch: 0, loss (training): 11.1516, loss (eval): 11.2019
Epoch: 1, loss (training): 11.1466, loss (eval): 11.1484
Epoch: 2, loss (training): 11.1465, loss (eval): 11.1495
Epoch: 3, loss (training): 11.1474, loss (eval): 11.1465
Epoch: 4, loss (training): 11.1498, loss (eval): 11.1498
Epoch: 5, loss (training): 11.1478, loss (eval): 11.1481
Epoch: 6, loss (training): 11.148, loss (eval): 11.1504
Epoch: 7, loss (training): 11.1463, loss (eval): 11.143
Epoch: 8, loss (training): 11.1479, loss (eval): 11.1469
Epoch: 9, loss (training): 11.1487, loss (eval): 11.1477
Epoch: 10, loss (training): 11.147, loss (eval): 11.1388
Epoch: 11, loss (training): 11.1489, loss (eval): 11.151
Epoch: 12, loss (training): 11.1474, loss (eval): 11.1432
Epoch: 13, loss (training): 11.147, loss (eval): 11.1391
Epoch: 14, loss (training): 11.1466, loss (eval): 11.1429
Epoch: 15, loss (training): 11.15, loss (eval): 11.1413
Epoch: 16, loss (training): 11.1483, loss (eval): 11.142
Epoch: 17, loss (training): 11.1472, loss (eval): 11.1514
Epoch: 18, loss (training): 11.1479, loss (eval): 11.1573
Epoch: 19, loss (training): 11.148, loss (eval): 11.1592
Epoch: 20, loss (training): 11.1493, loss (eval): 11.1454
Epoch: 21, loss (training): 11.1492, loss (eval): 11.1767
Epoch: 22, loss (training): 11.1466, loss (eval): 11.1514
Epoch: 23, loss (training): 11.1468, loss (eval): 11.144
Epoch: 24, loss (training): 11.1462, loss (eval): 11.143
Iteration: 9
optimizer_post_lr: [0.001]
prob_prior: 0.001661557273173934
start update likelihood model
Epoch: 0, loss (training): 10.0502, loss (eval): 10.1445
Epoch: 1, loss (training): 10.0195, loss (eval): 10.1661
Epoch: 2, loss (training): 10.0539, loss (eval): 10.1643
Epoch: 3, loss (training): 10.0056, loss (eval): 10.166
Epoch: 4, loss (training): 10.0085, loss (eval): 10.1489
Epoch: 5, loss (training): 10.0004, loss (eval): 10.1134
Epoch: 6, loss (training): 10.0085, loss (eval): 10.1716
Epoch: 7, loss (training): 9.9912, loss (eval): 10.1478
Epoch: 8, loss (training): 9.9947, loss (eval): 10.1317
Epoch: 9, loss (training): 9.973, loss (eval): 10.1102
Epoch: 10, loss (training): 9.9961, loss (eval): 10.167
Epoch: 11, loss (training): 9.9697, loss (eval): 10.1459
Epoch: 12, loss (training): 9.9564, loss (eval): 10.1207
Epoch: 13, loss (training): 9.9518, loss (eval): 10.1234
Epoch: 14, loss (training): 9.9512, loss (eval): 10.1618
Epoch: 15, loss (training): 9.9645, loss (eval): 10.1589
Epoch: 16, loss (training): 9.9679, loss (eval): 10.1364
Epoch: 17, loss (training): 9.9393, loss (eval): 10.1583
Epoch: 18, loss (training): 9.9449, loss (eval): 10.1673
Epoch: 19, loss (training): 9.958, loss (eval): 10.1231
Epoch: 20, loss (training): 9.9409, loss (eval): 10.0927
Epoch: 21, loss (training): 9.928, loss (eval): 10.1622
Epoch: 22, loss (training): 9.9528, loss (eval): 10.2352
Epoch: 23, loss (training): 9.9453, loss (eval): 10.1821
Epoch: 24, loss (training): 9.9264, loss (eval): 10.1534
start update posterior model
Epoch: 0, loss (training): 11.1488, loss (eval): 11.1974
Epoch: 1, loss (training): 11.1435, loss (eval): 11.1398
Epoch: 2, loss (training): 11.1451, loss (eval): 11.1302
Epoch: 3, loss (training): 11.1447, loss (eval): 11.1298
Epoch: 4, loss (training): 11.1426, loss (eval): 11.1463
Epoch: 5, loss (training): 11.1466, loss (eval): 11.1461
Epoch: 6, loss (training): 11.1448, loss (eval): 11.1441
Epoch: 7, loss (training): 11.1421, loss (eval): 11.1369
Epoch: 8, loss (training): 11.1423, loss (eval): 11.1334
Epoch: 9, loss (training): 11.1421, loss (eval): 11.1426
Epoch: 10, loss (training): 11.1421, loss (eval): 11.1424
Epoch: 11, loss (training): 11.1438, loss (eval): 11.1743
Epoch: 12, loss (training): 11.143, loss (eval): 11.1533
Epoch: 13, loss (training): 11.1445, loss (eval): 11.1398
Epoch: 14, loss (training): 11.145, loss (eval): 11.136
Epoch: 15, loss (training): 11.1427, loss (eval): 11.1408
Epoch: 16, loss (training): 11.1411, loss (eval): 11.1456
Epoch: 17, loss (training): 11.1425, loss (eval): 11.142
Epoch: 18, loss (training): 11.1436, loss (eval): 11.1456
Epoch: 19, loss (training): 11.1431, loss (eval): 11.1446
Epoch: 20, loss (training): 11.1413, loss (eval): 11.1313
Epoch: 21, loss (training): 11.144, loss (eval): 11.1441
Epoch: 22, loss (training): 11.1425, loss (eval): 11.1395
Early-stopping. Training converged after 23 epochs.
Iteration: 10
optimizer_post_lr: [0.001]
prob_prior: 0.0007465858083766792
start update likelihood model
Epoch: 0, loss (training): 10.1658, loss (eval): 10.0611
Epoch: 1, loss (training): 10.1328, loss (eval): 10.052
Epoch: 2, loss (training): 10.1149, loss (eval): 10.0523
Epoch: 3, loss (training): 10.1165, loss (eval): 10.1176
Epoch: 4, loss (training): 10.0984, loss (eval): 10.1138
Epoch: 5, loss (training): 10.1188, loss (eval): 10.1489
Epoch: 6, loss (training): 10.0884, loss (eval): 10.0836
Epoch: 7, loss (training): 10.0836, loss (eval): 10.0856
Epoch: 8, loss (training): 10.0793, loss (eval): 10.0848
Epoch: 9, loss (training): 10.064, loss (eval): 10.0548
Epoch: 10, loss (training): 10.0658, loss (eval): 10.1172
Epoch: 11, loss (training): 10.058, loss (eval): 10.046
Epoch: 12, loss (training): 10.0605, loss (eval): 10.0818
Epoch: 13, loss (training): 10.0514, loss (eval): 10.0584
Epoch: 14, loss (training): 10.0353, loss (eval): 10.0987
Epoch: 15, loss (training): 10.0552, loss (eval): 10.1021
Epoch: 16, loss (training): 10.0604, loss (eval): 10.1127
Epoch: 17, loss (training): 10.0556, loss (eval): 10.1346
Epoch: 18, loss (training): 10.0435, loss (eval): 10.1175
Epoch: 19, loss (training): 10.051, loss (eval): 10.0616
Epoch: 20, loss (training): 10.0475, loss (eval): 10.0285
Epoch: 21, loss (training): 10.0383, loss (eval): 10.1131
Epoch: 22, loss (training): 10.0321, loss (eval): 10.1466
Epoch: 23, loss (training): 10.0327, loss (eval): 10.1175
Epoch: 24, loss (training): 10.0205, loss (eval): 10.087
start update posterior model
Epoch: 0, loss (training): 10.9341, loss (eval): 10.98
Epoch: 1, loss (training): 10.9287, loss (eval): 10.9464
Epoch: 2, loss (training): 10.9287, loss (eval): 10.9258
Epoch: 3, loss (training): 10.932, loss (eval): 10.9316
Epoch: 4, loss (training): 10.9278, loss (eval): 10.931
Epoch: 5, loss (training): 10.9319, loss (eval): 10.9256
Epoch: 6, loss (training): 10.9307, loss (eval): 10.9313
Epoch: 7, loss (training): 10.9276, loss (eval): 10.9277
Epoch: 8, loss (training): 10.9301, loss (eval): 10.9196
Epoch: 9, loss (training): 10.9301, loss (eval): 10.9339
Epoch: 10, loss (training): 10.9309, loss (eval): 10.9246
Epoch: 11, loss (training): 10.9285, loss (eval): 10.9253
Epoch: 12, loss (training): 10.9267, loss (eval): 10.9242
Epoch: 13, loss (training): 10.9251, loss (eval): 10.9182
Epoch: 14, loss (training): 10.9305, loss (eval): 10.9388
Epoch: 15, loss (training): 10.9295, loss (eval): 10.9208
Epoch: 16, loss (training): 10.9297, loss (eval): 10.9349
Epoch: 17, loss (training): 10.9305, loss (eval): 10.9372
Epoch: 18, loss (training): 10.929, loss (eval): 10.9279
Epoch: 19, loss (training): 10.9285, loss (eval): 10.9269
Epoch: 20, loss (training): 10.9283, loss (eval): 10.9348
Epoch: 21, loss (training): 10.9307, loss (eval): 10.9369
Epoch: 22, loss (training): 10.9316, loss (eval): 10.9284
Epoch: 23, loss (training): 10.9279, loss (eval): 10.9222
Epoch: 24, loss (training): 10.927, loss (eval): 10.9202

Runtime:954.2
0
1
2
3
4
5
6
7
8
9
