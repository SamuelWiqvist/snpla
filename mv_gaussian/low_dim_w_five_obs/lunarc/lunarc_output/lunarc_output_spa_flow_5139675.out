Input args:
Dim: 2
seed: 6
seed_data: 10
/home/samwiq/spa/seq-posterior-approx-w-nf-dev/mv_gaussian/low_dim_w_five_obs/lunarc
/home/samwiq/spa/seq-posterior-approx-w-nf-dev
[1.0, 0.4965853037914095, 0.2465969639416065, 0.12245642825298195, 0.06081006262521797, 0.0301973834223185, 0.014995576820477717, 0.007446583070924344, 0.003697863716482932, 0.0018363047770289071]
start full training
Iteration: 1
optimizer_post_lr: [0.002]
prob_prior: 1.0
start update likelihood model
Epoch: 0, loss (training): 26.4197, loss (eval): 43.7012
Epoch: 1, loss (training): 20.0091, loss (eval): 22.1685
Epoch: 2, loss (training): 17.8889, loss (eval): 19.4466
Epoch: 3, loss (training): 16.6286, loss (eval): 17.8542
Epoch: 4, loss (training): 15.5412, loss (eval): 16.6507
Epoch: 5, loss (training): 14.5246, loss (eval): 15.5993
Epoch: 6, loss (training): 13.551, loss (eval): 14.3993
Epoch: 7, loss (training): 12.7877, loss (eval): 13.4592
Epoch: 8, loss (training): 12.2793, loss (eval): 12.8481
Epoch: 9, loss (training): 11.7107, loss (eval): 12.2133
Epoch: 10, loss (training): 11.3348, loss (eval): 11.7104
Epoch: 11, loss (training): 11.0656, loss (eval): 11.4431
Epoch: 12, loss (training): 10.8535, loss (eval): 11.2202
Epoch: 13, loss (training): 10.6978, loss (eval): 10.9686
Epoch: 14, loss (training): 10.5911, loss (eval): 10.8012
Epoch: 15, loss (training): 10.454, loss (eval): 10.728
Epoch: 16, loss (training): 10.4147, loss (eval): 10.7589
Epoch: 17, loss (training): 10.4394, loss (eval): 10.5861
Epoch: 18, loss (training): 10.3845, loss (eval): 10.7332
Epoch: 19, loss (training): 10.3296, loss (eval): 10.5739
Epoch: 20, loss (training): 10.4124, loss (eval): 10.7191
Epoch: 21, loss (training): 10.2976, loss (eval): 10.7195
Epoch: 22, loss (training): 10.2825, loss (eval): 10.5789
Epoch: 23, loss (training): 10.287, loss (eval): 10.5387
Epoch: 24, loss (training): 10.2529, loss (eval): 10.7641
Epoch: 25, loss (training): 10.2312, loss (eval): 10.4782
Epoch: 26, loss (training): 10.2528, loss (eval): 10.6253
Epoch: 27, loss (training): 10.2289, loss (eval): 10.3944
Epoch: 28, loss (training): 10.1801, loss (eval): 10.5525
Epoch: 29, loss (training): 10.1885, loss (eval): 10.5181
Epoch: 30, loss (training): 10.1662, loss (eval): 10.3911
Epoch: 31, loss (training): 10.1567, loss (eval): 10.6083
Epoch: 32, loss (training): 10.2262, loss (eval): 10.6227
Epoch: 33, loss (training): 10.2294, loss (eval): 10.4688
Epoch: 34, loss (training): 10.1935, loss (eval): 10.4503
Epoch: 35, loss (training): 10.1786, loss (eval): 10.559
Epoch: 36, loss (training): 10.1353, loss (eval): 10.4664
Epoch: 37, loss (training): 10.1736, loss (eval): 10.5957
Epoch: 38, loss (training): 10.0886, loss (eval): 10.4455
Epoch: 39, loss (training): 10.129, loss (eval): 10.5319
Epoch: 40, loss (training): 10.1236, loss (eval): 10.3843
Epoch: 41, loss (training): 10.1859, loss (eval): 10.3926
Epoch: 42, loss (training): 10.0774, loss (eval): 10.4205
Epoch: 43, loss (training): 10.1285, loss (eval): 10.5051
Epoch: 44, loss (training): 10.1083, loss (eval): 10.5217
Epoch: 45, loss (training): 10.1232, loss (eval): 10.4198
Epoch: 46, loss (training): 10.0615, loss (eval): 10.4245
Epoch: 47, loss (training): 10.1007, loss (eval): 10.4043
Epoch: 48, loss (training): 10.0742, loss (eval): 10.3333
Epoch: 49, loss (training): 10.0575, loss (eval): 10.4439
Epoch: 50, loss (training): 10.1103, loss (eval): 10.3943
Epoch: 51, loss (training): 10.1308, loss (eval): 10.5239
Epoch: 52, loss (training): 10.1325, loss (eval): 10.3558
Epoch: 53, loss (training): 10.1691, loss (eval): 10.5314
Epoch: 54, loss (training): 10.0626, loss (eval): 10.3662
Epoch: 55, loss (training): 10.0615, loss (eval): 10.4323
Epoch: 56, loss (training): 10.1233, loss (eval): 10.3662
Epoch: 57, loss (training): 10.0457, loss (eval): 10.535
Epoch: 58, loss (training): 10.0699, loss (eval): 10.3347
Epoch: 59, loss (training): 10.0317, loss (eval): 10.3605
Epoch: 60, loss (training): 10.0099, loss (eval): 10.3599
Epoch: 61, loss (training): 10.0736, loss (eval): 10.5484
Epoch: 62, loss (training): 10.0095, loss (eval): 10.3627
Epoch: 63, loss (training): 10.0423, loss (eval): 10.3942
Epoch: 64, loss (training): 10.0136, loss (eval): 10.4481
Epoch: 65, loss (training): 10.0533, loss (eval): 10.3876
Epoch: 66, loss (training): 10.0225, loss (eval): 10.3885
Epoch: 67, loss (training): 9.982, loss (eval): 10.4411
Early-stopping. Training converged after 68 epochs.
start update posterior model from prior pred - hot start
Epoch: 0, loss (training): 3.622, loss (eval): 7.2322
Epoch: 1, loss (training): 1.6985, loss (eval): 2.4492
Epoch: 2, loss (training): 1.5273, loss (eval): 1.8508
Epoch: 3, loss (training): 1.3337, loss (eval): 1.8702
Epoch: 4, loss (training): 0.9326, loss (eval): 1.3031
Epoch: 5, loss (training): 0.7491, loss (eval): 0.9272
Epoch: 6, loss (training): 0.8804, loss (eval): 1.0183
Epoch: 7, loss (training): 0.6391, loss (eval): 0.6986
Epoch: 8, loss (training): 0.5951, loss (eval): 0.8433
Epoch: 9, loss (training): 0.6168, loss (eval): 0.6856
start update posterior model
Epoch: 0, loss (training): 18.9878, loss (eval): 19.5313
Epoch: 1, loss (training): 18.5868, loss (eval): 18.862
Epoch: 2, loss (training): 18.6233, loss (eval): 18.5323
Epoch: 3, loss (training): 18.5673, loss (eval): 18.5688
Epoch: 4, loss (training): 18.6105, loss (eval): 18.541
Epoch: 5, loss (training): 18.5765, loss (eval): 18.5447
Epoch: 6, loss (training): 18.5854, loss (eval): 18.5336
Epoch: 7, loss (training): 18.573, loss (eval): 18.568
Epoch: 8, loss (training): 18.5731, loss (eval): 18.546
Epoch: 9, loss (training): 18.5822, loss (eval): 18.5872
Epoch: 10, loss (training): 18.5658, loss (eval): 18.526
Epoch: 11, loss (training): 18.5639, loss (eval): 18.808
Epoch: 12, loss (training): 18.5589, loss (eval): 18.5417
Epoch: 13, loss (training): 18.5708, loss (eval): 18.5316
Epoch: 14, loss (training): 18.5762, loss (eval): 18.6884
Epoch: 15, loss (training): 18.5713, loss (eval): 18.5414
Epoch: 16, loss (training): 18.5672, loss (eval): 18.5538
Epoch: 17, loss (training): 18.5651, loss (eval): 18.5718
Epoch: 18, loss (training): 18.5848, loss (eval): 18.5518
Epoch: 19, loss (training): 18.5611, loss (eval): 18.5613
Epoch: 20, loss (training): 18.5671, loss (eval): 18.7186
Epoch: 21, loss (training): 18.5629, loss (eval): 18.5498
Epoch: 22, loss (training): 18.5522, loss (eval): 18.5478
Epoch: 23, loss (training): 18.5609, loss (eval): 18.5246
Epoch: 24, loss (training): 18.5556, loss (eval): 18.5774
Epoch: 25, loss (training): 18.5539, loss (eval): 18.5404
Epoch: 26, loss (training): 18.5582, loss (eval): 18.528
Epoch: 27, loss (training): 18.5723, loss (eval): 18.5338
Epoch: 28, loss (training): 18.5635, loss (eval): 18.5282
Epoch: 29, loss (training): 18.5529, loss (eval): 18.5538
Epoch: 30, loss (training): 18.5567, loss (eval): 18.5443
Epoch: 31, loss (training): 18.5592, loss (eval): 18.5588
Epoch: 32, loss (training): 18.5543, loss (eval): 18.5449
Epoch: 33, loss (training): 18.5596, loss (eval): 18.5401
Epoch: 34, loss (training): 18.5585, loss (eval): 18.5325
Epoch: 35, loss (training): 18.5478, loss (eval): 18.5349
Epoch: 36, loss (training): 18.5459, loss (eval): 18.568
Epoch: 37, loss (training): 18.5542, loss (eval): 18.5442
Epoch: 38, loss (training): 18.5651, loss (eval): 18.6237
Epoch: 39, loss (training): 18.5479, loss (eval): 18.5219
Epoch: 40, loss (training): 18.5488, loss (eval): 18.5273
Epoch: 41, loss (training): 18.5611, loss (eval): 18.5301
Epoch: 42, loss (training): 18.5426, loss (eval): 18.5576
Epoch: 43, loss (training): 18.5786, loss (eval): 18.5299
Epoch: 44, loss (training): 18.559, loss (eval): 18.619
Epoch: 45, loss (training): 18.5623, loss (eval): 18.6904
Epoch: 46, loss (training): 18.5509, loss (eval): 18.5764
Epoch: 47, loss (training): 18.5554, loss (eval): 18.5403
Epoch: 48, loss (training): 18.5537, loss (eval): 18.5491
Epoch: 49, loss (training): 18.5513, loss (eval): 18.5206
Epoch: 50, loss (training): 18.5654, loss (eval): 18.6006
Epoch: 51, loss (training): 18.5549, loss (eval): 18.5269
Epoch: 52, loss (training): 18.5609, loss (eval): 18.5654
Epoch: 53, loss (training): 18.5483, loss (eval): 18.5508
Epoch: 54, loss (training): 18.5526, loss (eval): 18.5586
Epoch: 55, loss (training): 18.5491, loss (eval): 18.5942
Epoch: 56, loss (training): 18.5543, loss (eval): 18.5443
Epoch: 57, loss (training): 18.5549, loss (eval): 18.5707
Epoch: 58, loss (training): 18.5481, loss (eval): 18.5621
Epoch: 59, loss (training): 18.5436, loss (eval): 18.5481
Epoch: 60, loss (training): 18.5566, loss (eval): 18.6151
Epoch: 61, loss (training): 18.5472, loss (eval): 18.6596
Epoch: 62, loss (training): 18.5506, loss (eval): 18.5376
Epoch: 63, loss (training): 18.5461, loss (eval): 18.5576
Epoch: 64, loss (training): 18.5473, loss (eval): 18.5257
Epoch: 65, loss (training): 18.5474, loss (eval): 18.5479
Epoch: 66, loss (training): 18.5509, loss (eval): 18.5361
Epoch: 67, loss (training): 18.543, loss (eval): 18.5331
Epoch: 68, loss (training): 18.5478, loss (eval): 18.5285
Early-stopping. Training converged after 69 epochs.
Iteration: 2
optimizer_post_lr: [0.0019]
prob_prior: 0.4965853037914095
start update likelihood model
Epoch: 0, loss (training): 10.4216, loss (eval): 10.6547
Epoch: 1, loss (training): 10.3824, loss (eval): 10.7125
Epoch: 2, loss (training): 10.2807, loss (eval): 10.509
Epoch: 3, loss (training): 10.2986, loss (eval): 10.4823
Epoch: 4, loss (training): 10.2792, loss (eval): 10.7345
Epoch: 5, loss (training): 10.2695, loss (eval): 10.3736
Epoch: 6, loss (training): 10.2231, loss (eval): 10.4233
Epoch: 7, loss (training): 10.2155, loss (eval): 10.326
Epoch: 8, loss (training): 10.1841, loss (eval): 10.3714
Epoch: 9, loss (training): 10.2209, loss (eval): 10.4736
Epoch: 10, loss (training): 10.2134, loss (eval): 10.3228
Epoch: 11, loss (training): 10.1709, loss (eval): 10.2983
Epoch: 12, loss (training): 10.1882, loss (eval): 10.4283
Epoch: 13, loss (training): 10.227, loss (eval): 10.3234
Epoch: 14, loss (training): 10.1167, loss (eval): 10.409
Epoch: 15, loss (training): 10.1515, loss (eval): 10.3592
Epoch: 16, loss (training): 10.1366, loss (eval): 10.5105
Epoch: 17, loss (training): 10.1411, loss (eval): 10.3509
Epoch: 18, loss (training): 10.2044, loss (eval): 10.3541
Epoch: 19, loss (training): 10.1209, loss (eval): 10.3765
Epoch: 20, loss (training): 10.1728, loss (eval): 10.3913
Epoch: 21, loss (training): 10.1129, loss (eval): 10.4591
Epoch: 22, loss (training): 10.1497, loss (eval): 10.2994
Epoch: 23, loss (training): 10.1358, loss (eval): 10.4022
Epoch: 24, loss (training): 10.1474, loss (eval): 10.3724
Epoch: 25, loss (training): 10.1182, loss (eval): 10.5623
Epoch: 26, loss (training): 10.1496, loss (eval): 10.4675
Epoch: 27, loss (training): 10.1529, loss (eval): 10.5501
Epoch: 28, loss (training): 10.0764, loss (eval): 10.3986
Epoch: 29, loss (training): 10.1128, loss (eval): 10.3946
Epoch: 30, loss (training): 10.0608, loss (eval): 10.4071
Early-stopping. Training converged after 31 epochs.
start update posterior model
Epoch: 0, loss (training): 20.0238, loss (eval): 20.5501
Epoch: 1, loss (training): 20.0084, loss (eval): 19.9833
Epoch: 2, loss (training): 19.9938, loss (eval): 19.9893
Epoch: 3, loss (training): 20.0183, loss (eval): 20.0164
Epoch: 4, loss (training): 20.0078, loss (eval): 19.9951
Epoch: 5, loss (training): 19.9949, loss (eval): 19.9954
Epoch: 6, loss (training): 19.9922, loss (eval): 20.0001
Epoch: 7, loss (training): 20.0007, loss (eval): 19.9838
Epoch: 8, loss (training): 19.9881, loss (eval): 19.9816
Epoch: 9, loss (training): 20.0037, loss (eval): 20.0275
Epoch: 10, loss (training): 19.9964, loss (eval): 19.9772
Epoch: 11, loss (training): 20.0066, loss (eval): 20.0085
Epoch: 12, loss (training): 19.994, loss (eval): 19.9803
Epoch: 13, loss (training): 19.9949, loss (eval): 19.9737
Epoch: 14, loss (training): 20.0062, loss (eval): 19.9876
Epoch: 15, loss (training): 19.9966, loss (eval): 20.0013
Epoch: 16, loss (training): 19.9929, loss (eval): 19.982
Epoch: 17, loss (training): 19.9954, loss (eval): 19.977
Epoch: 18, loss (training): 19.9936, loss (eval): 19.9794
Epoch: 19, loss (training): 20.0008, loss (eval): 19.9864
Epoch: 20, loss (training): 20.0039, loss (eval): 20.0123
Epoch: 21, loss (training): 19.9984, loss (eval): 19.9879
Epoch: 22, loss (training): 19.9989, loss (eval): 20.0076
Epoch: 23, loss (training): 19.9996, loss (eval): 19.9746
Epoch: 24, loss (training): 20.0035, loss (eval): 19.9842
Epoch: 25, loss (training): 19.9898, loss (eval): 19.9917
Epoch: 26, loss (training): 19.9906, loss (eval): 19.9704
Epoch: 27, loss (training): 19.9901, loss (eval): 19.9995
Epoch: 28, loss (training): 20.0004, loss (eval): 19.9763
Epoch: 29, loss (training): 19.9975, loss (eval): 19.9785
Epoch: 30, loss (training): 19.9965, loss (eval): 19.9811
Epoch: 31, loss (training): 19.9919, loss (eval): 19.9831
Epoch: 32, loss (training): 19.9954, loss (eval): 19.9699
Epoch: 33, loss (training): 19.9916, loss (eval): 19.9732
Epoch: 34, loss (training): 19.9949, loss (eval): 19.9806
Epoch: 35, loss (training): 19.9932, loss (eval): 19.995
Epoch: 36, loss (training): 19.9972, loss (eval): 19.9801
Epoch: 37, loss (training): 19.9971, loss (eval): 19.9804
Epoch: 38, loss (training): 20.0065, loss (eval): 20.0172
Epoch: 39, loss (training): 19.9935, loss (eval): 19.9966
Epoch: 40, loss (training): 19.9872, loss (eval): 19.9745
Epoch: 41, loss (training): 19.9905, loss (eval): 19.9822
Epoch: 42, loss (training): 19.993, loss (eval): 19.972
Epoch: 43, loss (training): 19.9946, loss (eval): 19.982
Epoch: 44, loss (training): 19.9923, loss (eval): 19.9729
Epoch: 45, loss (training): 19.993, loss (eval): 19.9769
Epoch: 46, loss (training): 19.9943, loss (eval): 19.9807
Epoch: 47, loss (training): 19.9991, loss (eval): 19.9826
Epoch: 48, loss (training): 19.9891, loss (eval): 19.981
Epoch: 49, loss (training): 19.994, loss (eval): 19.9721
Epoch: 50, loss (training): 19.9978, loss (eval): 19.991
Epoch: 51, loss (training): 19.9954, loss (eval): 19.9918
Early-stopping. Training converged after 52 epochs.
Iteration: 3
optimizer_post_lr: [0.001805]
prob_prior: 0.2465969639416065
start update likelihood model
Epoch: 0, loss (training): 10.3365, loss (eval): 10.5092
Epoch: 1, loss (training): 10.3255, loss (eval): 10.6782
Epoch: 2, loss (training): 10.3076, loss (eval): 10.5987
Epoch: 3, loss (training): 10.2621, loss (eval): 10.6795
Epoch: 4, loss (training): 10.2333, loss (eval): 10.5079
Epoch: 5, loss (training): 10.2196, loss (eval): 10.5944
Epoch: 6, loss (training): 10.268, loss (eval): 10.4791
Epoch: 7, loss (training): 10.1983, loss (eval): 10.5714
Epoch: 8, loss (training): 10.2324, loss (eval): 10.5383
Epoch: 9, loss (training): 10.2155, loss (eval): 10.455
Epoch: 10, loss (training): 10.188, loss (eval): 10.6865
Epoch: 11, loss (training): 10.1802, loss (eval): 10.4766
Epoch: 12, loss (training): 10.2022, loss (eval): 10.4212
Epoch: 13, loss (training): 10.1291, loss (eval): 10.4103
Epoch: 14, loss (training): 10.1737, loss (eval): 10.3897
Epoch: 15, loss (training): 10.1547, loss (eval): 10.4253
Epoch: 16, loss (training): 10.1689, loss (eval): 10.4202
Epoch: 17, loss (training): 10.1075, loss (eval): 10.5341
Epoch: 18, loss (training): 10.1193, loss (eval): 10.5057
Epoch: 19, loss (training): 10.1362, loss (eval): 10.4629
Epoch: 20, loss (training): 10.1549, loss (eval): 10.3912
Epoch: 21, loss (training): 10.1467, loss (eval): 10.5929
Epoch: 22, loss (training): 10.1554, loss (eval): 10.3857
Epoch: 23, loss (training): 10.2001, loss (eval): 10.4911
Epoch: 24, loss (training): 10.118, loss (eval): 10.5566
Epoch: 25, loss (training): 10.1382, loss (eval): 10.496
Epoch: 26, loss (training): 10.1589, loss (eval): 10.5469
Epoch: 27, loss (training): 10.1318, loss (eval): 10.4286
Epoch: 28, loss (training): 10.1403, loss (eval): 10.4691
Epoch: 29, loss (training): 10.1339, loss (eval): 10.3644
Epoch: 30, loss (training): 10.107, loss (eval): 10.4392
Epoch: 31, loss (training): 10.1082, loss (eval): 10.4825
Epoch: 32, loss (training): 10.0773, loss (eval): 10.4913
Epoch: 33, loss (training): 10.0905, loss (eval): 10.4413
Epoch: 34, loss (training): 10.075, loss (eval): 10.5289
Epoch: 35, loss (training): 10.076, loss (eval): 10.4068
Epoch: 36, loss (training): 10.0831, loss (eval): 10.4804
Epoch: 37, loss (training): 10.073, loss (eval): 10.5576
Epoch: 38, loss (training): 10.1284, loss (eval): 10.4733
Epoch: 39, loss (training): 10.1583, loss (eval): 10.5821
Epoch: 40, loss (training): 10.1252, loss (eval): 10.4612
Epoch: 41, loss (training): 10.1215, loss (eval): 10.5815
Epoch: 42, loss (training): 10.072, loss (eval): 10.4726
Epoch: 43, loss (training): 10.0808, loss (eval): 10.5114
Epoch: 44, loss (training): 10.1147, loss (eval): 10.483
Epoch: 45, loss (training): 10.0892, loss (eval): 10.5137
Epoch: 46, loss (training): 10.1007, loss (eval): 10.5488
Epoch: 47, loss (training): 10.0657, loss (eval): 10.4704
Epoch: 48, loss (training): 10.0642, loss (eval): 10.5259
Early-stopping. Training converged after 49 epochs.
start update posterior model
Epoch: 0, loss (training): 18.5488, loss (eval): 18.5514
Epoch: 1, loss (training): 18.5508, loss (eval): 18.5327
Epoch: 2, loss (training): 18.548, loss (eval): 18.5339
Epoch: 3, loss (training): 18.5526, loss (eval): 18.5597
Epoch: 4, loss (training): 18.5493, loss (eval): 18.5473
Epoch: 5, loss (training): 18.5512, loss (eval): 18.6052
Epoch: 6, loss (training): 18.5584, loss (eval): 18.5434
Epoch: 7, loss (training): 18.5558, loss (eval): 18.5371
Epoch: 8, loss (training): 18.549, loss (eval): 18.5348
Epoch: 9, loss (training): 18.5469, loss (eval): 18.5372
Epoch: 10, loss (training): 18.5482, loss (eval): 18.5595
Epoch: 11, loss (training): 18.5491, loss (eval): 18.5382
Epoch: 12, loss (training): 18.5533, loss (eval): 18.5546
Epoch: 13, loss (training): 18.545, loss (eval): 18.5329
Epoch: 14, loss (training): 18.5469, loss (eval): 18.5527
Epoch: 15, loss (training): 18.55, loss (eval): 18.5316
Epoch: 16, loss (training): 18.5447, loss (eval): 18.534
Epoch: 17, loss (training): 18.5476, loss (eval): 18.5381
Epoch: 18, loss (training): 18.5527, loss (eval): 18.5598
Epoch: 19, loss (training): 18.5461, loss (eval): 18.5402
Epoch: 20, loss (training): 18.5524, loss (eval): 18.5546
Epoch: 21, loss (training): 18.5544, loss (eval): 18.5628
Epoch: 22, loss (training): 18.5469, loss (eval): 18.5359
Epoch: 23, loss (training): 18.5447, loss (eval): 18.555
Epoch: 24, loss (training): 18.5495, loss (eval): 18.5996
Epoch: 25, loss (training): 18.5553, loss (eval): 18.5377
Epoch: 26, loss (training): 18.5521, loss (eval): 18.5415
Epoch: 27, loss (training): 18.5403, loss (eval): 18.5603
Epoch: 28, loss (training): 18.5483, loss (eval): 18.5337
Epoch: 29, loss (training): 18.5528, loss (eval): 18.5388
Epoch: 30, loss (training): 18.548, loss (eval): 18.577
Epoch: 31, loss (training): 18.5446, loss (eval): 18.5449
Epoch: 32, loss (training): 18.5445, loss (eval): 18.5769
Epoch: 33, loss (training): 18.551, loss (eval): 18.5362
Epoch: 34, loss (training): 18.5511, loss (eval): 18.5643
Early-stopping. Training converged after 35 epochs.
Iteration: 4
optimizer_post_lr: [0.00171475]
prob_prior: 0.12245642825298195
start update likelihood model
Epoch: 0, loss (training): 10.2634, loss (eval): 10.1743
Epoch: 1, loss (training): 10.2509, loss (eval): 10.0485
Epoch: 2, loss (training): 10.1554, loss (eval): 10.0652
Epoch: 3, loss (training): 10.156, loss (eval): 10.0398
Epoch: 4, loss (training): 10.1822, loss (eval): 10.0689
Epoch: 5, loss (training): 10.1749, loss (eval): 10.251
Epoch: 6, loss (training): 10.1231, loss (eval): 10.0869
Epoch: 7, loss (training): 10.1854, loss (eval): 9.9743
Epoch: 8, loss (training): 10.121, loss (eval): 10.1506
Epoch: 9, loss (training): 10.1359, loss (eval): 10.0243
Epoch: 10, loss (training): 10.0844, loss (eval): 10.0644
Epoch: 11, loss (training): 10.089, loss (eval): 10.1114
Epoch: 12, loss (training): 10.0622, loss (eval): 10.0173
Epoch: 13, loss (training): 10.1598, loss (eval): 10.2697
Epoch: 14, loss (training): 10.118, loss (eval): 10.1436
Epoch: 15, loss (training): 10.082, loss (eval): 10.0957
Epoch: 16, loss (training): 10.081, loss (eval): 10.0935
Epoch: 17, loss (training): 10.0591, loss (eval): 10.0551
Epoch: 18, loss (training): 10.0218, loss (eval): 10.0197
Epoch: 19, loss (training): 10.0767, loss (eval): 10.0655
Epoch: 20, loss (training): 10.0986, loss (eval): 10.166
Epoch: 21, loss (training): 10.0915, loss (eval): 10.0187
Epoch: 22, loss (training): 10.1224, loss (eval): 10.2141
Epoch: 23, loss (training): 10.1104, loss (eval): 10.2112
Epoch: 24, loss (training): 10.0425, loss (eval): 10.0343
Epoch: 25, loss (training): 10.0521, loss (eval): 10.0259
Epoch: 26, loss (training): 10.0351, loss (eval): 10.1156
Early-stopping. Training converged after 27 epochs.
start update posterior model
Epoch: 0, loss (training): 17.7891, loss (eval): 17.8443
Epoch: 1, loss (training): 17.8004, loss (eval): 17.7794
Epoch: 2, loss (training): 17.778, loss (eval): 17.7677
Epoch: 3, loss (training): 17.7812, loss (eval): 17.8097
Epoch: 4, loss (training): 17.7855, loss (eval): 17.7639
Epoch: 5, loss (training): 17.7887, loss (eval): 17.7711
Epoch: 6, loss (training): 17.7797, loss (eval): 17.7824
Epoch: 7, loss (training): 17.7735, loss (eval): 17.7697
Epoch: 8, loss (training): 17.7818, loss (eval): 17.8242
Epoch: 9, loss (training): 17.7758, loss (eval): 17.7635
Epoch: 10, loss (training): 17.7832, loss (eval): 17.7661
Epoch: 11, loss (training): 17.7836, loss (eval): 17.8042
Epoch: 12, loss (training): 17.7804, loss (eval): 17.8267
Epoch: 13, loss (training): 17.7786, loss (eval): 17.7702
Epoch: 14, loss (training): 17.7771, loss (eval): 17.7986
Epoch: 15, loss (training): 17.7754, loss (eval): 17.7644
Epoch: 16, loss (training): 17.7801, loss (eval): 17.78
Epoch: 17, loss (training): 17.778, loss (eval): 17.7797
Epoch: 18, loss (training): 17.7797, loss (eval): 17.8226
Epoch: 19, loss (training): 17.7839, loss (eval): 17.7904
Epoch: 20, loss (training): 17.7846, loss (eval): 17.7788
Epoch: 21, loss (training): 17.7875, loss (eval): 17.7743
Epoch: 22, loss (training): 17.7802, loss (eval): 17.7829
Epoch: 23, loss (training): 17.7842, loss (eval): 17.7681
Epoch: 24, loss (training): 17.7801, loss (eval): 17.8271
Epoch: 25, loss (training): 17.7787, loss (eval): 17.7686
Epoch: 26, loss (training): 17.775, loss (eval): 17.7788
Epoch: 27, loss (training): 17.7907, loss (eval): 17.7989
Epoch: 28, loss (training): 17.7771, loss (eval): 17.7699
Early-stopping. Training converged after 29 epochs.
Iteration: 5
optimizer_post_lr: [0.0016290124999999997]
prob_prior: 0.06081006262521797
start update likelihood model
Epoch: 0, loss (training): 10.1706, loss (eval): 9.9636
Epoch: 1, loss (training): 10.1903, loss (eval): 9.9017
Epoch: 2, loss (training): 10.171, loss (eval): 10.0037
Epoch: 3, loss (training): 10.0744, loss (eval): 9.9778
Epoch: 4, loss (training): 10.0992, loss (eval): 10.0189
Epoch: 5, loss (training): 10.0816, loss (eval): 10.0217
Epoch: 6, loss (training): 10.0612, loss (eval): 9.9273
Epoch: 7, loss (training): 10.1149, loss (eval): 9.9724
Epoch: 8, loss (training): 10.0605, loss (eval): 9.9719
Epoch: 9, loss (training): 10.0827, loss (eval): 10.0273
Epoch: 10, loss (training): 10.0821, loss (eval): 9.9653
Epoch: 11, loss (training): 10.1031, loss (eval): 9.9522
Epoch: 12, loss (training): 10.0702, loss (eval): 9.9428
Epoch: 13, loss (training): 10.016, loss (eval): 9.9753
Epoch: 14, loss (training): 10.0289, loss (eval): 10.026
Epoch: 15, loss (training): 10.0353, loss (eval): 9.9694
Epoch: 16, loss (training): 10.0585, loss (eval): 10.108
Epoch: 17, loss (training): 10.0522, loss (eval): 10.0472
Epoch: 18, loss (training): 10.034, loss (eval): 10.0248
Epoch: 19, loss (training): 10.0091, loss (eval): 9.9718
Epoch: 20, loss (training): 10.0202, loss (eval): 9.9499
Early-stopping. Training converged after 21 epochs.
start update posterior model
Epoch: 0, loss (training): 19.844, loss (eval): 20.57
Epoch: 1, loss (training): 19.829, loss (eval): 19.8128
Epoch: 2, loss (training): 19.8213, loss (eval): 19.8118
Epoch: 3, loss (training): 19.8205, loss (eval): 19.8126
Epoch: 4, loss (training): 19.8254, loss (eval): 19.82
Epoch: 5, loss (training): 19.8254, loss (eval): 19.8188
Epoch: 6, loss (training): 19.8247, loss (eval): 19.8168
Epoch: 7, loss (training): 19.8282, loss (eval): 19.8485
Epoch: 8, loss (training): 19.8214, loss (eval): 19.8102
Epoch: 9, loss (training): 19.8211, loss (eval): 19.8338
Epoch: 10, loss (training): 19.8248, loss (eval): 19.8293
Epoch: 11, loss (training): 19.821, loss (eval): 19.8218
Epoch: 12, loss (training): 19.8269, loss (eval): 19.8158
Epoch: 13, loss (training): 19.826, loss (eval): 19.8397
Epoch: 14, loss (training): 19.8221, loss (eval): 19.8099
Epoch: 15, loss (training): 19.8229, loss (eval): 19.8228
Epoch: 16, loss (training): 19.8265, loss (eval): 19.8142
Epoch: 17, loss (training): 19.8233, loss (eval): 19.8396
Epoch: 18, loss (training): 19.8187, loss (eval): 19.8293
Epoch: 19, loss (training): 19.8314, loss (eval): 19.833
Epoch: 20, loss (training): 19.8194, loss (eval): 19.826
Epoch: 21, loss (training): 19.8255, loss (eval): 19.814
Epoch: 22, loss (training): 19.823, loss (eval): 19.8464
Epoch: 23, loss (training): 19.8267, loss (eval): 19.8208
Epoch: 24, loss (training): 19.8194, loss (eval): 19.8149
Epoch: 25, loss (training): 19.8199, loss (eval): 19.8069
Epoch: 26, loss (training): 19.8204, loss (eval): 19.8171
Epoch: 27, loss (training): 19.8247, loss (eval): 19.8126
Epoch: 28, loss (training): 19.8242, loss (eval): 19.8443
Epoch: 29, loss (training): 19.8248, loss (eval): 19.8197
Epoch: 30, loss (training): 19.8199, loss (eval): 19.8667
Epoch: 31, loss (training): 19.8183, loss (eval): 19.8071
Epoch: 32, loss (training): 19.8334, loss (eval): 19.8197
Epoch: 33, loss (training): 19.8267, loss (eval): 19.8208
Epoch: 34, loss (training): 19.8244, loss (eval): 19.8172
Epoch: 35, loss (training): 19.8212, loss (eval): 19.8127
Epoch: 36, loss (training): 19.8263, loss (eval): 19.8237
Epoch: 37, loss (training): 19.8255, loss (eval): 19.8028
Epoch: 38, loss (training): 19.8226, loss (eval): 19.8194
Epoch: 39, loss (training): 19.8329, loss (eval): 19.8107
Epoch: 40, loss (training): 19.8259, loss (eval): 19.8168
Epoch: 41, loss (training): 19.8257, loss (eval): 19.809
Epoch: 42, loss (training): 19.8245, loss (eval): 19.8254
Epoch: 43, loss (training): 19.8305, loss (eval): 19.8134
Epoch: 44, loss (training): 19.8294, loss (eval): 19.8135
Epoch: 45, loss (training): 19.8224, loss (eval): 19.8086
Epoch: 46, loss (training): 19.8241, loss (eval): 19.8073
Epoch: 47, loss (training): 19.8225, loss (eval): 19.8119
Epoch: 48, loss (training): 19.8254, loss (eval): 19.8108
Epoch: 49, loss (training): 19.8293, loss (eval): 19.8246
Epoch: 50, loss (training): 19.8192, loss (eval): 19.8187
Epoch: 51, loss (training): 19.8191, loss (eval): 19.8457
Epoch: 52, loss (training): 19.8182, loss (eval): 19.8185
Epoch: 53, loss (training): 19.8182, loss (eval): 19.83
Epoch: 54, loss (training): 19.8247, loss (eval): 19.8197
Epoch: 55, loss (training): 19.8196, loss (eval): 19.8228
Epoch: 56, loss (training): 19.8193, loss (eval): 19.8769
Early-stopping. Training converged after 57 epochs.
Iteration: 6
optimizer_post_lr: [0.0015475618749999996]
prob_prior: 0.0301973834223185
start update likelihood model
Epoch: 0, loss (training): 10.0947, loss (eval): 10.1262
Epoch: 1, loss (training): 10.0895, loss (eval): 10.2599
Epoch: 2, loss (training): 10.038, loss (eval): 10.2231
Epoch: 3, loss (training): 10.0671, loss (eval): 10.193
Epoch: 4, loss (training): 10.0663, loss (eval): 10.2287
Epoch: 5, loss (training): 10.0716, loss (eval): 10.2603
Epoch: 6, loss (training): 10.0687, loss (eval): 10.3923
Epoch: 7, loss (training): 10.0512, loss (eval): 10.387
Epoch: 8, loss (training): 10.0175, loss (eval): 10.2664
Epoch: 9, loss (training): 10.0019, loss (eval): 10.3201
Epoch: 10, loss (training): 9.9796, loss (eval): 10.2063
Epoch: 11, loss (training): 10.0089, loss (eval): 10.2237
Epoch: 12, loss (training): 10.0002, loss (eval): 10.3008
Epoch: 13, loss (training): 10.0673, loss (eval): 10.3113
Epoch: 14, loss (training): 9.9704, loss (eval): 10.3328
Epoch: 15, loss (training): 9.9646, loss (eval): 10.2073
Epoch: 16, loss (training): 9.9912, loss (eval): 10.3238
Epoch: 17, loss (training): 10.0236, loss (eval): 10.2482
Epoch: 18, loss (training): 9.9764, loss (eval): 10.3929
Epoch: 19, loss (training): 9.9968, loss (eval): 10.294
Early-stopping. Training converged after 20 epochs.
start update posterior model
Epoch: 0, loss (training): 19.9877, loss (eval): 19.9916
Epoch: 1, loss (training): 19.9884, loss (eval): 19.9823
Epoch: 2, loss (training): 19.9868, loss (eval): 19.9916
Epoch: 3, loss (training): 19.9908, loss (eval): 19.976
Epoch: 4, loss (training): 19.981, loss (eval): 19.971
Epoch: 5, loss (training): 19.9817, loss (eval): 19.9816
Epoch: 6, loss (training): 19.9878, loss (eval): 19.9856
Epoch: 7, loss (training): 19.9833, loss (eval): 19.9801
Epoch: 8, loss (training): 19.9821, loss (eval): 19.9781
Epoch: 9, loss (training): 19.9813, loss (eval): 19.9781
Epoch: 10, loss (training): 19.9815, loss (eval): 19.9748
Epoch: 11, loss (training): 19.9873, loss (eval): 19.9815
Epoch: 12, loss (training): 19.9856, loss (eval): 19.9698
Epoch: 13, loss (training): 19.9829, loss (eval): 19.9724
Epoch: 14, loss (training): 19.9861, loss (eval): 19.9808
Epoch: 15, loss (training): 19.9824, loss (eval): 19.9752
Epoch: 16, loss (training): 19.9881, loss (eval): 19.9927
Epoch: 17, loss (training): 19.988, loss (eval): 20.0107
Epoch: 18, loss (training): 19.9841, loss (eval): 19.9737
Epoch: 19, loss (training): 19.983, loss (eval): 19.9749
Epoch: 20, loss (training): 19.9834, loss (eval): 19.9827
Epoch: 21, loss (training): 19.9796, loss (eval): 19.9744
Epoch: 22, loss (training): 19.9945, loss (eval): 20.0272
Epoch: 23, loss (training): 19.984, loss (eval): 19.9786
Epoch: 24, loss (training): 19.9823, loss (eval): 19.9687
Epoch: 25, loss (training): 19.9847, loss (eval): 19.9803
Epoch: 26, loss (training): 19.987, loss (eval): 20.0086
Epoch: 27, loss (training): 19.9819, loss (eval): 19.9747
Epoch: 28, loss (training): 19.9857, loss (eval): 20.0259
Epoch: 29, loss (training): 19.9899, loss (eval): 19.975
Epoch: 30, loss (training): 19.9806, loss (eval): 19.9756
Epoch: 31, loss (training): 19.9823, loss (eval): 19.9791
Epoch: 32, loss (training): 19.9826, loss (eval): 19.9794
Epoch: 33, loss (training): 19.9863, loss (eval): 20.0058
Epoch: 34, loss (training): 19.9796, loss (eval): 19.9739
Epoch: 35, loss (training): 19.9821, loss (eval): 19.989
Epoch: 36, loss (training): 19.9865, loss (eval): 19.992
Epoch: 37, loss (training): 19.9867, loss (eval): 19.99
Epoch: 38, loss (training): 19.9948, loss (eval): 19.9759
Epoch: 39, loss (training): 19.9838, loss (eval): 19.9877
Epoch: 40, loss (training): 19.9795, loss (eval): 19.9803
Epoch: 41, loss (training): 19.9836, loss (eval): 19.9842
Epoch: 42, loss (training): 19.981, loss (eval): 19.9949
Epoch: 43, loss (training): 19.9813, loss (eval): 19.9753
Early-stopping. Training converged after 44 epochs.
Iteration: 7
optimizer_post_lr: [0.0014701837812499995]
prob_prior: 0.014995576820477717
start update likelihood model
Epoch: 0, loss (training): 10.1201, loss (eval): 10.0973
Epoch: 1, loss (training): 10.0285, loss (eval): 10.0932
Epoch: 2, loss (training): 10.0709, loss (eval): 10.0862
Epoch: 3, loss (training): 10.0251, loss (eval): 10.0177
Epoch: 4, loss (training): 10.0034, loss (eval): 10.0854
Epoch: 5, loss (training): 10.0229, loss (eval): 10.0409
Epoch: 6, loss (training): 9.9726, loss (eval): 10.0331
Epoch: 7, loss (training): 9.975, loss (eval): 10.0691
Epoch: 8, loss (training): 10.0001, loss (eval): 10.2067
Epoch: 9, loss (training): 9.9778, loss (eval): 10.1503
Epoch: 10, loss (training): 9.9918, loss (eval): 10.1258
Epoch: 11, loss (training): 9.9532, loss (eval): 10.0687
Epoch: 12, loss (training): 9.9954, loss (eval): 10.2269
Epoch: 13, loss (training): 9.9555, loss (eval): 10.0784
Epoch: 14, loss (training): 9.9692, loss (eval): 10.0919
Epoch: 15, loss (training): 9.9949, loss (eval): 10.2259
Epoch: 16, loss (training): 9.9354, loss (eval): 10.1475
Epoch: 17, loss (training): 9.9402, loss (eval): 10.1886
Epoch: 18, loss (training): 10.0196, loss (eval): 10.2095
Epoch: 19, loss (training): 9.9665, loss (eval): 10.2072
Epoch: 20, loss (training): 9.9589, loss (eval): 10.1508
Epoch: 21, loss (training): 9.9374, loss (eval): 10.1533
Epoch: 22, loss (training): 9.9444, loss (eval): 10.2303
Early-stopping. Training converged after 23 epochs.
start update posterior model
Epoch: 0, loss (training): 19.1738, loss (eval): 19.2492
Epoch: 1, loss (training): 19.1682, loss (eval): 19.156
Epoch: 2, loss (training): 19.1656, loss (eval): 19.1882
Epoch: 3, loss (training): 19.1737, loss (eval): 19.16
Epoch: 4, loss (training): 19.1704, loss (eval): 19.1546
Epoch: 5, loss (training): 19.1691, loss (eval): 19.1588
Epoch: 6, loss (training): 19.1756, loss (eval): 19.1813
Epoch: 7, loss (training): 19.1633, loss (eval): 19.1632
Epoch: 8, loss (training): 19.1653, loss (eval): 19.1721
Epoch: 9, loss (training): 19.1688, loss (eval): 19.1616
Epoch: 10, loss (training): 19.1647, loss (eval): 19.1734
Epoch: 11, loss (training): 19.1665, loss (eval): 19.1739
Epoch: 12, loss (training): 19.1675, loss (eval): 19.1744
Epoch: 13, loss (training): 19.1636, loss (eval): 19.1558
Epoch: 14, loss (training): 19.1677, loss (eval): 19.1569
Epoch: 15, loss (training): 19.1663, loss (eval): 19.1664
Epoch: 16, loss (training): 19.1706, loss (eval): 19.1781
Epoch: 17, loss (training): 19.1652, loss (eval): 19.1665
Epoch: 18, loss (training): 19.1655, loss (eval): 19.1576
Epoch: 19, loss (training): 19.1658, loss (eval): 19.1766
Epoch: 20, loss (training): 19.164, loss (eval): 19.1553
Epoch: 21, loss (training): 19.1643, loss (eval): 19.1609
Epoch: 22, loss (training): 19.1648, loss (eval): 19.1702
Epoch: 23, loss (training): 19.1662, loss (eval): 19.1943
Early-stopping. Training converged after 24 epochs.
Iteration: 8
optimizer_post_lr: [0.0013966745921874994]
prob_prior: 0.007446583070924344
start update likelihood model
Epoch: 0, loss (training): 10.1713, loss (eval): 10.2142
Epoch: 1, loss (training): 10.1727, loss (eval): 10.2003
Epoch: 2, loss (training): 10.1521, loss (eval): 10.2264
Epoch: 3, loss (training): 10.0905, loss (eval): 10.191
Epoch: 4, loss (training): 10.0903, loss (eval): 10.1948
Epoch: 5, loss (training): 10.1086, loss (eval): 10.2279
Epoch: 6, loss (training): 10.1131, loss (eval): 10.2718
Epoch: 7, loss (training): 10.1346, loss (eval): 10.2688
Epoch: 8, loss (training): 10.1051, loss (eval): 10.2452
Epoch: 9, loss (training): 10.084, loss (eval): 10.1616
Epoch: 10, loss (training): 10.1016, loss (eval): 10.1767
Epoch: 11, loss (training): 10.1152, loss (eval): 10.2111
Epoch: 12, loss (training): 10.1081, loss (eval): 10.2697
Epoch: 13, loss (training): 10.0832, loss (eval): 10.2609
Epoch: 14, loss (training): 10.0835, loss (eval): 10.2405
Epoch: 15, loss (training): 10.0697, loss (eval): 10.2307
Epoch: 16, loss (training): 10.0775, loss (eval): 10.2201
Epoch: 17, loss (training): 10.0773, loss (eval): 10.2666
Epoch: 18, loss (training): 10.1117, loss (eval): 10.2348
Epoch: 19, loss (training): 10.0702, loss (eval): 10.2088
Epoch: 20, loss (training): 10.0788, loss (eval): 10.2331
Epoch: 21, loss (training): 10.0893, loss (eval): 10.2088
Epoch: 22, loss (training): 10.0809, loss (eval): 10.2044
Epoch: 23, loss (training): 10.0715, loss (eval): 10.1853
Epoch: 24, loss (training): 10.0939, loss (eval): 10.1653
Epoch: 25, loss (training): 10.0853, loss (eval): 10.243
Epoch: 26, loss (training): 10.0752, loss (eval): 10.1634
Epoch: 27, loss (training): 10.0599, loss (eval): 10.221
Epoch: 28, loss (training): 10.0607, loss (eval): 10.1834
Early-stopping. Training converged after 29 epochs.
start update posterior model
Epoch: 0, loss (training): 18.8912, loss (eval): 19.1477
Epoch: 1, loss (training): 18.8784, loss (eval): 18.8748
Epoch: 2, loss (training): 18.8801, loss (eval): 18.8719
Epoch: 3, loss (training): 18.8806, loss (eval): 18.8697
Epoch: 4, loss (training): 18.8769, loss (eval): 18.8751
Epoch: 5, loss (training): 18.8758, loss (eval): 18.8737
Epoch: 6, loss (training): 18.8756, loss (eval): 18.8749
Epoch: 7, loss (training): 18.8758, loss (eval): 18.8708
Epoch: 8, loss (training): 18.8802, loss (eval): 18.8687
Epoch: 9, loss (training): 18.8748, loss (eval): 18.8656
Epoch: 10, loss (training): 18.883, loss (eval): 18.8748
Epoch: 11, loss (training): 18.8757, loss (eval): 18.875
Epoch: 12, loss (training): 18.8777, loss (eval): 18.8796
Epoch: 13, loss (training): 18.8733, loss (eval): 18.8717
Epoch: 14, loss (training): 18.8791, loss (eval): 18.8646
Epoch: 15, loss (training): 18.8748, loss (eval): 18.8718
Epoch: 16, loss (training): 18.8794, loss (eval): 18.9341
Epoch: 17, loss (training): 18.8833, loss (eval): 18.8718
Epoch: 18, loss (training): 18.8778, loss (eval): 18.8689
Epoch: 19, loss (training): 18.8785, loss (eval): 18.8762
Epoch: 20, loss (training): 18.8751, loss (eval): 18.8701
Epoch: 21, loss (training): 18.8794, loss (eval): 18.8754
Epoch: 22, loss (training): 18.8761, loss (eval): 18.8789
Epoch: 23, loss (training): 18.8753, loss (eval): 18.8739
Epoch: 24, loss (training): 18.8836, loss (eval): 18.8732
Epoch: 25, loss (training): 18.8756, loss (eval): 18.8786
Epoch: 26, loss (training): 18.8809, loss (eval): 18.8837
Epoch: 27, loss (training): 18.8753, loss (eval): 18.8812
Epoch: 28, loss (training): 18.8766, loss (eval): 18.8698
Epoch: 29, loss (training): 18.8752, loss (eval): 18.8661
Epoch: 30, loss (training): 18.8784, loss (eval): 18.8689
Epoch: 31, loss (training): 18.883, loss (eval): 18.8759
Epoch: 32, loss (training): 18.8772, loss (eval): 18.909
Epoch: 33, loss (training): 18.8745, loss (eval): 18.8687
Early-stopping. Training converged after 34 epochs.
Iteration: 9
optimizer_post_lr: [0.0013268408625781243]
prob_prior: 0.003697863716482932
start update likelihood model
Epoch: 0, loss (training): 10.1389, loss (eval): 10.262
Epoch: 1, loss (training): 10.0741, loss (eval): 10.2529
Epoch: 2, loss (training): 10.0649, loss (eval): 10.2298
Epoch: 3, loss (training): 10.0278, loss (eval): 10.2167
Epoch: 4, loss (training): 10.0602, loss (eval): 10.3218
Epoch: 5, loss (training): 10.0713, loss (eval): 10.3143
Epoch: 6, loss (training): 10.0166, loss (eval): 10.2903
Epoch: 7, loss (training): 10.0394, loss (eval): 10.2116
Epoch: 8, loss (training): 10.0121, loss (eval): 10.2307
Epoch: 9, loss (training): 10.0289, loss (eval): 10.3287
Epoch: 10, loss (training): 10.0376, loss (eval): 10.2869
Epoch: 11, loss (training): 10.0133, loss (eval): 10.278
Epoch: 12, loss (training): 10.0211, loss (eval): 10.3108
Epoch: 13, loss (training): 10.0055, loss (eval): 10.2477
Epoch: 14, loss (training): 10.025, loss (eval): 10.2515
Epoch: 15, loss (training): 10.0231, loss (eval): 10.299
Epoch: 16, loss (training): 10.0228, loss (eval): 10.2886
Epoch: 17, loss (training): 10.0129, loss (eval): 10.2687
Epoch: 18, loss (training): 10.0232, loss (eval): 10.3275
Epoch: 19, loss (training): 10.0002, loss (eval): 10.2893
Epoch: 20, loss (training): 10.0112, loss (eval): 10.283
Epoch: 21, loss (training): 9.9949, loss (eval): 10.3371
Epoch: 22, loss (training): 10.0279, loss (eval): 10.295
Epoch: 23, loss (training): 10.019, loss (eval): 10.2836
Epoch: 24, loss (training): 9.9771, loss (eval): 10.342
Epoch: 25, loss (training): 9.9892, loss (eval): 10.3136
Epoch: 26, loss (training): 10.0213, loss (eval): 10.4074
Early-stopping. Training converged after 27 epochs.
start update posterior model
Epoch: 0, loss (training): 19.0668, loss (eval): 19.2191
Epoch: 1, loss (training): 19.0576, loss (eval): 19.0606
Epoch: 2, loss (training): 19.0639, loss (eval): 19.0668
Epoch: 3, loss (training): 19.0593, loss (eval): 19.0865
Epoch: 4, loss (training): 19.0622, loss (eval): 19.0601
Epoch: 5, loss (training): 19.0627, loss (eval): 19.0559
Epoch: 6, loss (training): 19.0608, loss (eval): 19.1049
Epoch: 7, loss (training): 19.0629, loss (eval): 19.0613
Epoch: 8, loss (training): 19.058, loss (eval): 19.0703
Epoch: 9, loss (training): 19.0639, loss (eval): 19.0741
Epoch: 10, loss (training): 19.0646, loss (eval): 19.0662
Epoch: 11, loss (training): 19.0584, loss (eval): 19.0635
Epoch: 12, loss (training): 19.062, loss (eval): 19.062
Epoch: 13, loss (training): 19.0595, loss (eval): 19.0645
Epoch: 14, loss (training): 19.0615, loss (eval): 19.0603
Epoch: 15, loss (training): 19.0627, loss (eval): 19.0557
Epoch: 16, loss (training): 19.0658, loss (eval): 19.07
Epoch: 17, loss (training): 19.0582, loss (eval): 19.0802
Epoch: 18, loss (training): 19.0649, loss (eval): 19.0589
Epoch: 19, loss (training): 19.0579, loss (eval): 19.0712
Epoch: 20, loss (training): 19.0621, loss (eval): 19.063
Epoch: 21, loss (training): 19.0595, loss (eval): 19.0514
Epoch: 22, loss (training): 19.0611, loss (eval): 19.062
Epoch: 23, loss (training): 19.0593, loss (eval): 19.0586
Epoch: 24, loss (training): 19.0619, loss (eval): 19.0574
Epoch: 25, loss (training): 19.0634, loss (eval): 19.0507
Epoch: 26, loss (training): 19.0612, loss (eval): 19.049
Epoch: 27, loss (training): 19.0614, loss (eval): 19.0567
Epoch: 28, loss (training): 19.0607, loss (eval): 19.0538
Epoch: 29, loss (training): 19.0603, loss (eval): 19.0675
Epoch: 30, loss (training): 19.0683, loss (eval): 19.0897
Epoch: 31, loss (training): 19.0668, loss (eval): 19.079
Epoch: 32, loss (training): 19.0616, loss (eval): 19.0537
Epoch: 33, loss (training): 19.0588, loss (eval): 19.0709
Epoch: 34, loss (training): 19.0647, loss (eval): 19.0543
Epoch: 35, loss (training): 19.0646, loss (eval): 19.0586
Epoch: 36, loss (training): 19.0648, loss (eval): 19.0614
Epoch: 37, loss (training): 19.0627, loss (eval): 19.0739
Epoch: 38, loss (training): 19.0633, loss (eval): 19.1308
Epoch: 39, loss (training): 19.0612, loss (eval): 19.0537
Epoch: 40, loss (training): 19.0644, loss (eval): 19.0674
Epoch: 41, loss (training): 19.0589, loss (eval): 19.0552
Epoch: 42, loss (training): 19.0649, loss (eval): 19.0576
Epoch: 43, loss (training): 19.0624, loss (eval): 19.0628
Epoch: 44, loss (training): 19.0603, loss (eval): 19.0662
Epoch: 45, loss (training): 19.0575, loss (eval): 19.0566
Early-stopping. Training converged after 46 epochs.
Iteration: 10
optimizer_post_lr: [0.001260498819449218]
prob_prior: 0.0018363047770289071
start update likelihood model
Epoch: 0, loss (training): 10.2158, loss (eval): 10.1688
Epoch: 1, loss (training): 10.1733, loss (eval): 10.1182
Epoch: 2, loss (training): 10.1452, loss (eval): 10.1041
Epoch: 3, loss (training): 10.1476, loss (eval): 10.1314
Epoch: 4, loss (training): 10.1234, loss (eval): 10.1169
Epoch: 5, loss (training): 10.1454, loss (eval): 10.163
Epoch: 6, loss (training): 10.1955, loss (eval): 10.3039
Epoch: 7, loss (training): 10.1439, loss (eval): 10.1002
Epoch: 8, loss (training): 10.1177, loss (eval): 10.1224
Epoch: 9, loss (training): 10.1372, loss (eval): 10.0908
Epoch: 10, loss (training): 10.1564, loss (eval): 10.2135
Epoch: 11, loss (training): 10.1428, loss (eval): 10.1378
Epoch: 12, loss (training): 10.1336, loss (eval): 10.1267
Epoch: 13, loss (training): 10.132, loss (eval): 10.0941
Epoch: 14, loss (training): 10.1072, loss (eval): 10.1261
Epoch: 15, loss (training): 10.1147, loss (eval): 10.1208
Epoch: 16, loss (training): 10.1351, loss (eval): 10.1497
Epoch: 17, loss (training): 10.1279, loss (eval): 10.1917
Epoch: 18, loss (training): 10.1099, loss (eval): 10.2003
Epoch: 19, loss (training): 10.0848, loss (eval): 10.1572
Epoch: 20, loss (training): 10.1212, loss (eval): 10.1052
Epoch: 21, loss (training): 10.0865, loss (eval): 10.1613
Epoch: 22, loss (training): 10.0713, loss (eval): 10.1833
Epoch: 23, loss (training): 10.1103, loss (eval): 10.0928
Epoch: 24, loss (training): 10.0918, loss (eval): 10.118
Epoch: 25, loss (training): 10.0855, loss (eval): 10.0861
Epoch: 26, loss (training): 10.1302, loss (eval): 10.1848
Epoch: 27, loss (training): 10.0961, loss (eval): 10.1053
Epoch: 28, loss (training): 10.0824, loss (eval): 10.0994
Epoch: 29, loss (training): 10.0757, loss (eval): 10.1081
Epoch: 30, loss (training): 10.0565, loss (eval): 10.1017
Epoch: 31, loss (training): 10.0678, loss (eval): 10.1268
Epoch: 32, loss (training): 10.0919, loss (eval): 10.1174
Epoch: 33, loss (training): 10.081, loss (eval): 10.0986
Epoch: 34, loss (training): 10.0718, loss (eval): 10.1586
Epoch: 35, loss (training): 10.0941, loss (eval): 10.1426
Epoch: 36, loss (training): 10.0823, loss (eval): 10.1167
Epoch: 37, loss (training): 10.099, loss (eval): 10.2019
Epoch: 38, loss (training): 10.0842, loss (eval): 10.1428
Epoch: 39, loss (training): 10.0768, loss (eval): 10.105
Epoch: 40, loss (training): 10.0884, loss (eval): 10.1336
Epoch: 41, loss (training): 10.0552, loss (eval): 10.1605
Epoch: 42, loss (training): 10.0407, loss (eval): 10.1092
Epoch: 43, loss (training): 10.0441, loss (eval): 10.1464
Epoch: 44, loss (training): 10.0465, loss (eval): 10.2236
Early-stopping. Training converged after 45 epochs.
start update posterior model
Epoch: 0, loss (training): 17.8915, loss (eval): 17.9029
Epoch: 1, loss (training): 17.895, loss (eval): 17.9051
Epoch: 2, loss (training): 17.893, loss (eval): 17.8904
Epoch: 3, loss (training): 17.898, loss (eval): 17.8983
Epoch: 4, loss (training): 17.8949, loss (eval): 17.8795
Epoch: 5, loss (training): 17.8954, loss (eval): 17.8761
Epoch: 6, loss (training): 17.8914, loss (eval): 17.8847
Epoch: 7, loss (training): 17.8905, loss (eval): 17.9256
Epoch: 8, loss (training): 17.8901, loss (eval): 17.8918
Epoch: 9, loss (training): 17.8927, loss (eval): 17.9006
Epoch: 10, loss (training): 17.8886, loss (eval): 17.882
Epoch: 11, loss (training): 17.8928, loss (eval): 17.8931
Epoch: 12, loss (training): 17.8944, loss (eval): 17.8868
Epoch: 13, loss (training): 17.8966, loss (eval): 17.8826
Epoch: 14, loss (training): 17.8897, loss (eval): 17.8883
Epoch: 15, loss (training): 17.8904, loss (eval): 17.8902
Epoch: 16, loss (training): 17.8916, loss (eval): 17.8902
Epoch: 17, loss (training): 17.89, loss (eval): 17.8958
Epoch: 18, loss (training): 17.8955, loss (eval): 17.8855
Epoch: 19, loss (training): 17.8939, loss (eval): 17.9092
Epoch: 20, loss (training): 17.8906, loss (eval): 17.8837
Epoch: 21, loss (training): 17.893, loss (eval): 17.8814
Epoch: 22, loss (training): 17.8918, loss (eval): 17.8973
Epoch: 23, loss (training): 17.8936, loss (eval): 17.8939
Epoch: 24, loss (training): 17.8887, loss (eval): 17.8951
Early-stopping. Training converged after 25 epochs.

Runtime:1628.46
0
1
2
3
4
5
6
7
8
9
